{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial train set size: 60000\n",
      "final train set size: 48000\n",
      "final valid set size: 12000\n",
      "batch size train: torch.Size([32, 1, 28, 28])\n",
      "batch size valid: torch.Size([32, 1, 28, 28])\n",
      "batch size test: torch.Size([32, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# loading a dataset from torchvision datasets\n",
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "\n",
    "#\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(30),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(\n",
    "root = 'datasets',\n",
    "train = True,\n",
    "transform = train_transforms,\n",
    "download = True,\n",
    ")\n",
    "test_dataset = datasets.MNIST(\n",
    "root = 'datasets',\n",
    "train = False,\n",
    "transform = test_transforms,\n",
    "download=True,\n",
    ")\n",
    "\n",
    "# break train data into train, validation\n",
    "train_len = int(len(train_dataset)*.80)\n",
    "valid_len = len(train_dataset) - train_len\n",
    "\n",
    "print(f\"initial train set size: {len(train_dataset)}\")\n",
    "train_dataset, validation_dataset = torch.utils.data.random_split(train_dataset, [train_len, valid_len])\n",
    "print(f\"final train set size: {len(train_dataset)}\")\n",
    "print(f\"final valid set size: {len(validation_dataset)}\")\n",
    "\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                          batch_size=32,\n",
    "                                          shuffle=True)\n",
    "\n",
    "validation_dataloader = torch.utils.data.DataLoader(validation_dataset,\n",
    "                                          batch_size=32,\n",
    "                                          shuffle=True)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                          batch_size=32,\n",
    "                                          shuffle=True)\n",
    "\n",
    "images, _ = next(iter(train_dataloader))\n",
    "print(f\"batch size train: {images.shape}\")\n",
    "images, _ = next(iter(validation_dataloader))\n",
    "print(f\"batch size valid: {images.shape}\")\n",
    "images, _ = next(iter(test_dataloader))\n",
    "print(f\"batch size test: {images.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# if you are loading images from a local folder you can do, where the images are individual jpg files\n",
    "# # Pass transforms in here, then run the next cell to see how the transforms look\n",
    "# train_data = datasets.ImageFolder(data_dir + '/train', transform=train_transforms)\n",
    "# test_data = datasets.ImageFolder(data_dir + '/test', transform=test_transforms)\n",
    "#\n",
    "# trainloader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "# testloader = torch.utils.data.DataLoader(test_data, batch_size=32, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 1000x400 with 4 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize some images\n",
    "import matplotlib.pyplot as plt\n",
    "import helper\n",
    "images, labels = next(iter(train_dataloader))\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(10,4), ncols=4)\n",
    "for ii in range(4):\n",
    "    ax = axes[ii]\n",
    "    helper.imshow(images[ii], ax=ax, normalize=False)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyCNN(\n",
      "  (cl1): Conv2d(1, 8, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
      "  (cl2): Conv2d(8, 16, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
      "  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (relu): ReLU()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (fc1): Linear(in_features=784, out_features=10, bias=True)\n",
      "  (logsoft): LogSoftmax(dim=1)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": "torch.Size([32, 10])"
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now define CNN model\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cl1 = nn.Conv2d(\n",
    "            in_channels=1,\n",
    "            out_channels=8,\n",
    "            kernel_size=2,\n",
    "            stride=1,\n",
    "            padding=1)\n",
    "\n",
    "        self.cl2 = nn.Conv2d(\n",
    "            in_channels=8,\n",
    "            out_channels=16,\n",
    "            kernel_size=2,\n",
    "            stride=1,\n",
    "            padding=1)\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(.1)\n",
    "        self.fc1 = nn.Linear(784, 10)\n",
    "        self.logsoft = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.maxpool(self.cl1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.maxpool(self.cl2(x)))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # flatten before feeding to linear layer\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.logsoft(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# try a forward pass to make sure everything looks OK\n",
    "model = MyCNN()\n",
    "print(model)\n",
    "\n",
    "images, labels = next(iter(train_dataloader))\n",
    "model(images).shape\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 2.399 val loss: 2.458 val acc: 0.082\n",
      "train loss: 0.971 val loss: 0.539 val acc: 0.828\n",
      "train loss: 0.769 val loss: 0.448 val acc: 0.854\n",
      "train loss: 0.339 val loss: 0.397 val acc: 0.875\n",
      "train loss: 0.463 val loss: 0.404 val acc: 0.869\n",
      "train loss: 0.454 val loss: 0.369 val acc: 0.877\n",
      "train loss: 0.686 val loss: 0.349 val acc: 0.885\n",
      "train loss: 0.413 val loss: 0.36 val acc: 0.882\n",
      "train loss: 0.416 val loss: 0.348 val acc: 0.885\n",
      "train loss: 0.565 val loss: 0.332 val acc: 0.893\n",
      "train loss: 0.404 val loss: 0.33 val acc: 0.89\n",
      "train loss: 0.405 val loss: 0.332 val acc: 0.891\n",
      "train loss: 0.537 val loss: 0.329 val acc: 0.889\n",
      "train loss: 0.401 val loss: 0.339 val acc: 0.887\n",
      "train loss: 0.398 val loss: 0.334 val acc: 0.892\n",
      "train loss: 0.418 val loss: 0.322 val acc: 0.894\n",
      "train loss: 0.383 val loss: 0.34 val acc: 0.89\n",
      "train loss: 0.388 val loss: 0.311 val acc: 0.898\n",
      "train loss: 0.517 val loss: 0.331 val acc: 0.892\n",
      "train loss: 0.381 val loss: 0.321 val acc: 0.895\n",
      "train loss: 0.384 val loss: 0.334 val acc: 0.89\n",
      "train loss: 0.287 val loss: 0.307 val acc: 0.897\n",
      "train loss: 0.386 val loss: 0.321 val acc: 0.896\n",
      "train loss: 0.382 val loss: 0.311 val acc: 0.9\n",
      "train loss: 0.52 val loss: 0.327 val acc: 0.89\n",
      "train loss: 0.382 val loss: 0.315 val acc: 0.9\n",
      "train loss: 0.376 val loss: 0.316 val acc: 0.898\n",
      "train loss: 0.396 val loss: 0.315 val acc: 0.897\n",
      "train loss: 0.368 val loss: 0.314 val acc: 0.898\n",
      "train loss: 0.369 val loss: 0.308 val acc: 0.899\n"
     ]
    }
   ],
   "source": [
    "# now lets train\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=.003)\n",
    "criteria = nn.NLLLoss()\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "def accuracy(model, criteria, dataset):\n",
    "\n",
    "    running_loss = 0\n",
    "    running_accuracy = 0\n",
    "    for idx, (images, labels) in enumerate(dataset):\n",
    "        images = images.to(\"mps\")\n",
    "        labels = labels.to(\"mps\")\n",
    "        model.eval()\n",
    "        log_soft = model.forward(images)\n",
    "        loss = criteria(log_soft, labels)\n",
    "        running_loss += loss.item()\n",
    "        _,preds = torch.exp(log_soft).topk(1)\n",
    "        equals = labels.squeeze()==preds.squeeze()\n",
    "        running_accuracy += torch.mean(equals.to(torch.float32)).item()\n",
    "        model.train()\n",
    "\n",
    "    avg_acc = round(running_accuracy/len(dataset),3)\n",
    "    avg_los = round(running_loss/len(dataset),3)\n",
    "\n",
    "    return avg_acc, avg_los\n",
    "\n",
    "for e in range(1, epochs+1):\n",
    "\n",
    "    running_loss = 0\n",
    "\n",
    "    for idx, (images, labels) in enumerate(train_dataloader):\n",
    "\n",
    "        # use GPU\n",
    "        torch.device(\"mps\")\n",
    "        model.to(\"mps\")\n",
    "        images = images.to(\"mps\")\n",
    "        labels = labels.to(\"mps\")\n",
    "\n",
    "        # clean gradients\n",
    "        optimizer.zero_grad()\n",
    "        #forward\n",
    "        log_soft = model.forward(images)\n",
    "        # loss\n",
    "        loss = criteria(log_soft, labels)\n",
    "        running_loss+= loss.item()\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        # gradient step\n",
    "        optimizer.step()\n",
    "\n",
    "        #  evaluate\n",
    "        if idx%500==0:\n",
    "            train_loss = round(running_loss/(idx+1),3)\n",
    "            acc, loss_val = accuracy(model, criteria, validation_dataloader)\n",
    "            print(f\"train loss: {train_loss} val loss: {loss_val} val acc: {acc}\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
