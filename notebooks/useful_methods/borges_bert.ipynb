{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting torchsummary\n",
      "  Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\n",
      "Installing collected packages: torchsummary\n",
      "Successfully installed torchsummary-1.5.1\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p38/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#!pip install transformers\n",
    "#!pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import transformers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertGenerationEncoder, BertGenerationDecoder, EncoderDecoderModel, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type bert to instantiate a model of type bert-generation. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertGenerationEncoder: ['cls.seq_relationship.weight', 'bert.pooler.dense.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'bert.pooler.dense.weight', 'bert.embeddings.token_type_embeddings.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertGenerationEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertGenerationEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "You are using a model of type bert to instantiate a model of type bert-generation. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertGenerationDecoder: ['cls.seq_relationship.weight', 'bert.pooler.dense.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'bert.pooler.dense.weight', 'bert.embeddings.token_type_embeddings.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertGenerationDecoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertGenerationDecoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertGenerationDecoder were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.19.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.self.key.bias', 'bert.encoder.layer.23.crossattention.output.dense.weight', 'bert.encoder.layer.20.crossattention.self.value.weight', 'bert.encoder.layer.9.crossattention.self.key.weight', 'bert.encoder.layer.21.crossattention.self.value.bias', 'bert.encoder.layer.14.crossattention.self.query.weight', 'bert.encoder.layer.12.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.self.key.weight', 'bert.encoder.layer.18.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.15.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.12.crossattention.self.query.weight', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.19.crossattention.self.value.weight', 'bert.encoder.layer.15.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.22.crossattention.self.query.bias', 'bert.encoder.layer.21.crossattention.self.value.weight', 'bert.encoder.layer.15.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.18.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.22.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.7.crossattention.output.dense.weight', 'bert.encoder.layer.7.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.18.crossattention.self.query.bias', 'bert.encoder.layer.5.crossattention.self.value.bias', 'bert.encoder.layer.21.crossattention.self.query.weight', 'bert.encoder.layer.21.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.output.dense.bias', 'bert.encoder.layer.3.crossattention.output.dense.bias', 'bert.encoder.layer.14.crossattention.self.key.bias', 'bert.encoder.layer.16.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.self.query.weight', 'bert.encoder.layer.13.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.13.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.15.crossattention.self.query.weight', 'bert.encoder.layer.14.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.14.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.12.crossattention.self.query.bias', 'bert.encoder.layer.20.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.23.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.14.crossattention.self.key.weight', 'bert.encoder.layer.18.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.18.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.20.crossattention.self.value.bias', 'bert.encoder.layer.22.crossattention.self.value.weight', 'bert.encoder.layer.14.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.18.crossattention.self.key.weight', 'bert.encoder.layer.16.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.23.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.3.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.15.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.18.crossattention.self.value.weight', 'bert.encoder.layer.7.crossattention.self.key.weight', 'bert.encoder.layer.13.crossattention.self.value.weight', 'bert.encoder.layer.13.crossattention.self.query.bias', 'bert.encoder.layer.12.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.18.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.23.crossattention.self.value.bias', 'bert.encoder.layer.20.crossattention.self.key.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.self.value.weight', 'bert.encoder.layer.20.crossattention.self.query.bias', 'bert.encoder.layer.23.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.11.crossattention.self.value.bias', 'bert.encoder.layer.19.crossattention.self.key.bias', 'bert.encoder.layer.14.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.5.crossattention.self.query.weight', 'bert.encoder.layer.17.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.13.crossattention.self.value.bias', 'bert.encoder.layer.15.crossattention.self.key.weight', 'bert.encoder.layer.23.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.17.crossattention.self.key.bias', 'bert.encoder.layer.17.crossattention.output.dense.bias', 'bert.encoder.layer.19.crossattention.output.dense.weight', 'bert.encoder.layer.11.crossattention.output.dense.bias', 'bert.encoder.layer.11.crossattention.self.query.weight', 'bert.encoder.layer.12.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.13.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.14.crossattention.output.dense.weight', 'bert.encoder.layer.19.crossattention.self.value.bias', 'bert.encoder.layer.16.crossattention.self.key.bias', 'bert.encoder.layer.13.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.7.crossattention.self.query.bias', 'bert.encoder.layer.19.crossattention.self.query.weight', 'bert.encoder.layer.12.crossattention.output.dense.bias', 'bert.encoder.layer.15.crossattention.self.value.weight', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.23.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.12.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.16.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.self.query.bias', 'bert.encoder.layer.21.crossattention.output.dense.bias', 'bert.encoder.layer.17.crossattention.self.value.bias', 'bert.encoder.layer.19.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.5.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.20.crossattention.output.dense.weight', 'bert.encoder.layer.18.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.18.crossattention.self.value.bias', 'bert.encoder.layer.7.crossattention.self.value.weight', 'bert.encoder.layer.3.crossattention.self.value.weight', 'bert.encoder.layer.23.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.19.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.15.crossattention.self.value.bias', 'bert.encoder.layer.15.crossattention.output.dense.weight', 'bert.encoder.layer.16.crossattention.self.value.weight', 'bert.encoder.layer.9.crossattention.self.value.bias', 'bert.encoder.layer.16.crossattention.self.key.weight', 'bert.encoder.layer.12.crossattention.self.value.weight', 'bert.encoder.layer.7.crossattention.self.query.weight', 'bert.encoder.layer.17.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.crossattention.self.key.bias', 'bert.encoder.layer.20.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.20.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.17.crossattention.self.value.weight', 'bert.encoder.layer.17.crossattention.self.query.weight', 'bert.encoder.layer.12.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.19.crossattention.self.key.weight', 'bert.encoder.layer.14.crossattention.self.value.weight', 'bert.encoder.layer.23.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.20.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.22.crossattention.self.value.bias', 'bert.encoder.layer.13.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.22.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.9.crossattention.output.dense.bias', 'bert.encoder.layer.21.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.21.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.22.crossattention.self.key.bias', 'bert.encoder.layer.17.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.22.crossattention.self.key.weight', 'bert.encoder.layer.22.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.21.crossattention.self.key.weight', 'lm_head.decoder.bias', 'bert.encoder.layer.21.crossattention.self.key.bias', 'bert.encoder.layer.19.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.9.crossattention.output.dense.weight', 'bert.encoder.layer.13.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.self.key.weight', 'bert.encoder.layer.17.crossattention.output.dense.weight', 'bert.encoder.layer.17.crossattention.self.query.bias', 'bert.encoder.layer.16.crossattention.self.query.bias', 'bert.encoder.layer.22.crossattention.self.query.weight', 'bert.encoder.layer.9.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.21.crossattention.output.dense.weight', 'bert.encoder.layer.3.crossattention.self.query.weight', 'lm_head.decoder.weight', 'bert.encoder.layer.5.crossattention.self.query.bias', 'bert.encoder.layer.5.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.23.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.5.crossattention.self.value.weight', 'bert.encoder.layer.16.crossattention.output.dense.bias', 'bert.encoder.layer.12.crossattention.output.dense.weight', 'lm_head.bias', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.20.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.14.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.16.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.5.crossattention.output.dense.weight', 'bert.encoder.layer.9.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.output.dense.weight', 'bert.encoder.layer.15.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.13.crossattention.self.query.weight', 'bert.encoder.layer.22.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.self.key.bias', 'bert.encoder.layer.16.crossattention.output.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a01298b49d6d45b9b473a891c81a0658",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c31ffcb599e4382a147ed75794b1b9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# leverage checkpoints for Bert2Bert model...\n",
    "# use BERT's cls token as BOS token and sep token as EOS token\n",
    "encoder = BertGenerationEncoder.from_pretrained(\"bert-large-uncased\", bos_token_id=101, eos_token_id=102)\n",
    "# add cross attention layers and use BERT's cls token as BOS token and sep token as EOS token\n",
    "decoder = BertGenerationDecoder.from_pretrained(\n",
    "    \"bert-large-uncased\", add_cross_attention=True, is_decoder=True, bos_token_id=101, eos_token_id=102\n",
    ")\n",
    "bert2bert = EncoderDecoderModel(encoder=encoder, decoder=decoder)\n",
    "\n",
    "# create tokenizer...\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-large-uncased\")\n",
    "\n",
    "input_ids = tokenizer(\n",
    "    \"This is a long article to summarize\", add_special_tokens=False, return_tensors=\"pt\"\n",
    ").input_ids\n",
    "labels = tokenizer(\"This is a short summary\", return_tensors=\"pt\").input_ids\n",
    "\n",
    "# train...\n",
    "loss = bert2bert(input_ids=input_ids, decoder_input_ids=labels, labels=labels).loss\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce5981da15664b55b1a4afb2144456ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/3.44k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9563b73b973844488a9e95684d5cf667",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.82G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'AutoTokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_18835/1741627477.py\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# instantiate sentence fusion model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msentence_fuser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoderDecoderModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"google/roberta2roberta_L-24_discofuse\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"google/roberta2roberta_L-24_discofuse\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m input_ids = tokenizer(\n",
      "\u001b[0;31mNameError\u001b[0m: name 'AutoTokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# instantiate sentence fusion model\n",
    "sentence_fuser = EncoderDecoderModel.from_pretrained(\"google/roberta2roberta_L-24_discofuse\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/roberta2roberta_L-24_discofuse\")\n",
    "\n",
    "input_ids = tokenizer(\n",
    "    \"This is the first sentence. This is the second sentence.\", add_special_tokens=False, return_tensors=\"pt\"\n",
    ").input_ids\n",
    "\n",
    "outputs = sentence_fuser.generate(input_ids)\n",
    "\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertDataset(Dataset):\n",
    "    def __init__(self, tokenizer,max_length):\n",
    "        super(BertDataset, self).__init__()        \n",
    "        self.root_dir=\"./\"\n",
    "        self.train_csv=pd.read_csv('https://github.com/clairett/pytorch-sentiment-classification/raw/master/data/SST2/train.tsv', delimiter='\\t', header=None)\n",
    "        self.tokenizer=tokenizer\n",
    "        self.target=self.train_csv.iloc[:,1]\n",
    "        self.max_length=max_length\n",
    "        \n",
    "\n",
    "    def load_words(self):\n",
    "        with open('./datasets/borges.txt','r',encoding='utf8') as f:\n",
    "            text = f.read()        \n",
    "            text = ''.join([i for i in text if i.isalpha() or i.isspace()])\n",
    "        if self.char_level==True:\n",
    "            return list(text)\n",
    "        else:\n",
    "            return text.split(' ')\n",
    "               \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.train_csv)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        text1 = self.train_csv.iloc[index,0]\n",
    "        \n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text1 ,\n",
    "            None,\n",
    "            pad_to_max_length=True,\n",
    "            add_special_tokens=True,\n",
    "            return_attention_mask=True,\n",
    "            max_length=self.max_length,\n",
    "        )\n",
    "        ids = inputs[\"input_ids\"]\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "        mask = inputs[\"attention_mask\"]\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'target': torch.tensor(self.train_csv.iloc[index, 1], dtype=torch.long)\n",
    "            }\n",
    "    \n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "dataset= BertDataset(tokenizer, max_length=100)\n",
    "dataloader=DataLoader(dataset=dataset,batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "class BERT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERT, self).__init__()\n",
    "        self.bert_model = transformers.BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.out = nn.Linear(768, 1)\n",
    "        \n",
    "    def forward(self,ids,mask,token_type_ids):\n",
    "        _,o2= self.bert_model(ids,attention_mask=mask,token_type_ids=token_type_ids, return_dict=False)\n",
    "        \n",
    "        out= self.out(o2)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "model=BERT()\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "#Initialize Optimizer\n",
    "optimizer= optim.Adam(model.parameters(),lr= 0.0001)\n",
    "\n",
    "# we are not retraining the weights of the BERT model, but only using them \n",
    "for param in model.bert_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': tensor([  101,  1037, 18385,  1010,  6057,  1998,  2633, 18276,  2128, 16603,\n",
       "          1997,  5053,  1998,  1996,  6841,  1998,  5687,  5469,  3152,   102,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]),\n",
       " 'mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0]),\n",
       " 'target': tensor(1)}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data =  next(iter(dataset))\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune(epochs,dataloader,model,loss_fn,optimizer):\n",
    "    model.train()\n",
    "    for  epoch in range(epochs):\n",
    "        print(epoch)\n",
    "        \n",
    "        loop=tqdm(enumerate(dataloader),leave=False,total=len(dataloader))\n",
    "        for batch, dl in loop:\n",
    "            ids=dl['ids']\n",
    "            token_type_ids=dl['token_type_ids']\n",
    "            mask= dl['mask']\n",
    "            label=dl['target']\n",
    "            label = label.unsqueeze(1)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output=model(\n",
    "                ids=ids,\n",
    "                mask=mask,\n",
    "                token_type_ids=token_type_ids)\n",
    "            label = label.type_as(output)\n",
    "\n",
    "            loss=loss_fn(output,label)\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            pred = np.where(output >= 0, 1, 0)\n",
    "\n",
    "            num_correct = sum(1 for a, b in zip(pred, label) if a[0] == b[0])\n",
    "            num_samples = pred.shape[0]\n",
    "            accuracy = num_correct/num_samples\n",
    "            \n",
    "            print(f'Got {num_correct} / {num_samples} with accuracy {float(num_correct)/float(num_samples)*100:.2f}')\n",
    "            \n",
    "            # Show progress while training\n",
    "            loop.set_description(f'Epoch={epoch}/{epochs}')\n",
    "            loop.set_postfix(loss=loss.item(),acc=accuracy)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/217 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2304: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Epoch=0/1:   0%|          | 1/217 [00:03<11:30,  3.20s/it, acc=0.5, loss=0.72]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 16 / 32 with accuracy 50.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:   1%|          | 2/217 [00:05<09:32,  2.66s/it, acc=0.75, loss=0.625]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 24 / 32 with accuracy 75.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:   1%|▏         | 3/217 [00:07<08:53,  2.49s/it, acc=0.594, loss=0.665]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 19 / 32 with accuracy 59.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:   2%|▏         | 4/217 [00:10<08:35,  2.42s/it, acc=0.594, loss=0.683]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 19 / 32 with accuracy 59.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:   2%|▏         | 5/217 [00:12<08:24,  2.38s/it, acc=0.531, loss=0.698]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 17 / 32 with accuracy 53.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:   3%|▎         | 6/217 [00:14<08:21,  2.38s/it, acc=0.594, loss=0.685]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 19 / 32 with accuracy 59.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:   3%|▎         | 7/217 [00:17<08:13,  2.35s/it, acc=0.562, loss=0.686]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 18 / 32 with accuracy 56.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:   4%|▎         | 8/217 [00:19<08:09,  2.34s/it, acc=0.625, loss=0.65] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 20 / 32 with accuracy 62.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:   4%|▍         | 9/217 [00:21<08:04,  2.33s/it, acc=0.656, loss=0.658]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 21 / 32 with accuracy 65.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:   5%|▍         | 10/217 [00:24<08:01,  2.33s/it, acc=0.5, loss=0.697] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 16 / 32 with accuracy 50.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:   5%|▌         | 11/217 [00:26<07:58,  2.32s/it, acc=0.531, loss=0.681]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 17 / 32 with accuracy 53.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:   6%|▌         | 12/217 [00:28<07:55,  2.32s/it, acc=0.469, loss=0.716]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 15 / 32 with accuracy 46.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:   6%|▌         | 13/217 [00:30<07:52,  2.31s/it, acc=0.688, loss=0.667]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 22 / 32 with accuracy 68.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:   6%|▋         | 14/217 [00:33<07:49,  2.31s/it, acc=0.5, loss=0.686]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 16 / 32 with accuracy 50.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:   7%|▋         | 15/217 [00:35<07:47,  2.31s/it, acc=0.406, loss=0.721]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 13 / 32 with accuracy 40.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:   7%|▋         | 16/217 [00:37<07:44,  2.31s/it, acc=0.562, loss=0.657]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 18 / 32 with accuracy 56.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:   8%|▊         | 17/217 [00:40<07:41,  2.31s/it, acc=0.438, loss=0.711]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 14 / 32 with accuracy 43.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:   8%|▊         | 18/217 [00:42<07:42,  2.32s/it, acc=0.469, loss=0.718]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 15 / 32 with accuracy 46.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:   9%|▉         | 19/217 [00:44<07:38,  2.31s/it, acc=0.594, loss=0.676]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 19 / 32 with accuracy 59.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:   9%|▉         | 20/217 [00:47<07:35,  2.31s/it, acc=0.719, loss=0.649]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 23 / 32 with accuracy 71.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:  10%|▉         | 21/217 [00:49<07:31,  2.31s/it, acc=0.594, loss=0.704]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 19 / 32 with accuracy 59.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:  10%|█         | 22/217 [00:51<07:30,  2.31s/it, acc=0.531, loss=0.695]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 17 / 32 with accuracy 53.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:  11%|█         | 23/217 [00:54<07:27,  2.31s/it, acc=0.625, loss=0.663]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 20 / 32 with accuracy 62.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:  11%|█         | 24/217 [00:56<07:24,  2.30s/it, acc=0.562, loss=0.692]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 18 / 32 with accuracy 56.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:  12%|█▏        | 25/217 [00:58<07:22,  2.30s/it, acc=0.438, loss=0.697]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 14 / 32 with accuracy 43.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:  12%|█▏        | 26/217 [01:00<07:20,  2.31s/it, acc=0.5, loss=0.698]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 16 / 32 with accuracy 50.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:  12%|█▏        | 27/217 [01:03<07:18,  2.31s/it, acc=0.719, loss=0.653]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 23 / 32 with accuracy 71.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:  13%|█▎        | 28/217 [01:05<07:15,  2.30s/it, acc=0.438, loss=0.697]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 14 / 32 with accuracy 43.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:  13%|█▎        | 29/217 [01:07<07:13,  2.31s/it, acc=0.688, loss=0.665]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 22 / 32 with accuracy 68.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:  14%|█▍        | 30/217 [01:10<07:11,  2.31s/it, acc=0.531, loss=0.687]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 17 / 32 with accuracy 53.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:  14%|█▍        | 31/217 [01:12<07:09,  2.31s/it, acc=0.625, loss=0.672]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 20 / 32 with accuracy 62.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:  15%|█▍        | 32/217 [01:15<07:41,  2.49s/it, acc=0.531, loss=0.693]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 17 / 32 with accuracy 53.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:  15%|█▌        | 33/217 [01:18<08:14,  2.69s/it, acc=0.5, loss=0.701]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 16 / 32 with accuracy 50.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:  16%|█▌        | 34/217 [01:20<07:49,  2.57s/it, acc=0.469, loss=0.698]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 15 / 32 with accuracy 46.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:  16%|█▌        | 35/217 [01:23<07:33,  2.49s/it, acc=0.531, loss=0.686]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 17 / 32 with accuracy 53.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:  17%|█▋        | 36/217 [01:25<07:20,  2.43s/it, acc=0.562, loss=0.687]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 18 / 32 with accuracy 56.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:  17%|█▋        | 37/217 [01:27<07:10,  2.39s/it, acc=0.438, loss=0.693]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 14 / 32 with accuracy 43.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:  18%|█▊        | 38/217 [01:30<07:02,  2.36s/it, acc=0.562, loss=0.678]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 18 / 32 with accuracy 56.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:  18%|█▊        | 39/217 [01:32<06:57,  2.35s/it, acc=0.469, loss=0.706]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 15 / 32 with accuracy 46.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:  18%|█▊        | 40/217 [01:34<06:52,  2.33s/it, acc=0.531, loss=0.693]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 17 / 32 with accuracy 53.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:  19%|█▉        | 41/217 [01:36<06:48,  2.32s/it, acc=0.531, loss=0.677]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 17 / 32 with accuracy 53.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:  19%|█▉        | 42/217 [01:39<06:45,  2.31s/it, acc=0.5, loss=0.696]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 16 / 32 with accuracy 50.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:  20%|█▉        | 43/217 [01:41<06:43,  2.32s/it, acc=0.594, loss=0.683]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 19 / 32 with accuracy 59.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:  20%|██        | 44/217 [01:43<06:39,  2.31s/it, acc=0.406, loss=0.699]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 13 / 32 with accuracy 40.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:  21%|██        | 45/217 [01:46<06:37,  2.31s/it, acc=0.656, loss=0.68] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 21 / 32 with accuracy 65.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:  21%|██        | 46/217 [01:48<06:34,  2.31s/it, acc=0.438, loss=0.687]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 14 / 32 with accuracy 43.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:  22%|██▏       | 47/217 [01:50<06:33,  2.32s/it, acc=0.688, loss=0.677]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 22 / 32 with accuracy 68.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:  22%|██▏       | 48/217 [01:53<06:30,  2.31s/it, acc=0.531, loss=0.689]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 17 / 32 with accuracy 53.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:  23%|██▎       | 49/217 [01:55<06:27,  2.31s/it, acc=0.5, loss=0.698]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 16 / 32 with accuracy 50.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:  23%|██▎       | 50/217 [01:57<06:25,  2.31s/it, acc=0.75, loss=0.647]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 24 / 32 with accuracy 75.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:  24%|██▎       | 51/217 [02:00<06:22,  2.31s/it, acc=0.531, loss=0.692]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 17 / 32 with accuracy 53.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:  24%|██▍       | 52/217 [02:02<06:25,  2.34s/it, acc=0.469, loss=0.71] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 15 / 32 with accuracy 46.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:  24%|██▍       | 53/217 [02:04<06:20,  2.32s/it, acc=0.781, loss=0.643]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 25 / 32 with accuracy 78.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:  25%|██▍       | 54/217 [02:07<06:17,  2.31s/it, acc=0.531, loss=0.699]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 17 / 32 with accuracy 53.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:  25%|██▌       | 55/217 [02:09<06:13,  2.31s/it, acc=0.5, loss=0.692]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 16 / 32 with accuracy 50.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:  26%|██▌       | 56/217 [02:11<06:10,  2.30s/it, acc=0.562, loss=0.678]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 18 / 32 with accuracy 56.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:  26%|██▋       | 57/217 [02:13<06:08,  2.30s/it, acc=0.75, loss=0.659] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 24 / 32 with accuracy 75.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:  27%|██▋       | 58/217 [02:16<06:05,  2.30s/it, acc=0.469, loss=0.701]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 15 / 32 with accuracy 46.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:  27%|██▋       | 59/217 [02:18<06:03,  2.30s/it, acc=0.531, loss=0.681]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 17 / 32 with accuracy 53.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:  28%|██▊       | 60/217 [02:20<06:01,  2.30s/it, acc=0.656, loss=0.673]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 21 / 32 with accuracy 65.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:  28%|██▊       | 61/217 [02:23<05:59,  2.30s/it, acc=0.469, loss=0.704]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 15 / 32 with accuracy 46.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:  29%|██▊       | 62/217 [02:25<05:57,  2.31s/it, acc=0.531, loss=0.675]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 17 / 32 with accuracy 53.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:  29%|██▉       | 63/217 [02:27<05:56,  2.31s/it, acc=0.469, loss=0.707]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 15 / 32 with accuracy 46.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:  29%|██▉       | 64/217 [02:30<05:53,  2.31s/it, acc=0.594, loss=0.658]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 19 / 32 with accuracy 59.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:  30%|██▉       | 65/217 [02:32<05:50,  2.30s/it, acc=0.594, loss=0.681]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 19 / 32 with accuracy 59.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:  30%|███       | 66/217 [02:34<05:47,  2.30s/it, acc=0.594, loss=0.672]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 19 / 32 with accuracy 59.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:  31%|███       | 67/217 [02:36<05:45,  2.30s/it, acc=0.625, loss=0.682]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 20 / 32 with accuracy 62.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:  31%|███▏      | 68/217 [02:39<05:43,  2.31s/it, acc=0.531, loss=0.695]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 17 / 32 with accuracy 53.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:  32%|███▏      | 69/217 [02:41<05:41,  2.31s/it, acc=0.625, loss=0.688]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 20 / 32 with accuracy 62.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:  32%|███▏      | 70/217 [02:43<05:40,  2.32s/it, acc=0.531, loss=0.692]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 17 / 32 with accuracy 53.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:  33%|███▎      | 71/217 [02:46<05:37,  2.31s/it, acc=0.469, loss=0.706]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 15 / 32 with accuracy 46.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:  33%|███▎      | 72/217 [02:48<05:34,  2.31s/it, acc=0.562, loss=0.706]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 18 / 32 with accuracy 56.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:  34%|███▎      | 73/217 [02:50<05:32,  2.31s/it, acc=0.469, loss=0.708]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 15 / 32 with accuracy 46.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:  34%|███▍      | 74/217 [02:53<05:28,  2.30s/it, acc=0.562, loss=0.679]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 18 / 32 with accuracy 56.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:  35%|███▍      | 75/217 [02:55<05:26,  2.30s/it, acc=0.344, loss=0.728]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 11 / 32 with accuracy 34.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:  35%|███▌      | 76/217 [02:57<05:24,  2.30s/it, acc=0.594, loss=0.677]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 19 / 32 with accuracy 59.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:  35%|███▌      | 77/217 [03:00<05:24,  2.32s/it, acc=0.469, loss=0.701]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 15 / 32 with accuracy 46.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:  36%|███▌      | 78/217 [03:02<05:21,  2.31s/it, acc=0.469, loss=0.704]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 15 / 32 with accuracy 46.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:  36%|███▋      | 79/217 [03:04<05:18,  2.31s/it, acc=0.5, loss=0.702]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 16 / 32 with accuracy 50.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:  37%|███▋      | 80/217 [03:06<05:15,  2.30s/it, acc=0.75, loss=0.653]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 24 / 32 with accuracy 75.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:  37%|███▋      | 81/217 [03:09<05:13,  2.30s/it, acc=0.719, loss=0.664]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 23 / 32 with accuracy 71.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:  38%|███▊      | 82/217 [03:11<05:11,  2.31s/it, acc=0.781, loss=0.64] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 25 / 32 with accuracy 78.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:  38%|███▊      | 83/217 [03:13<05:09,  2.31s/it, acc=0.469, loss=0.687]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 15 / 32 with accuracy 46.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:  39%|███▊      | 84/217 [03:16<05:08,  2.32s/it, acc=0.719, loss=0.66] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 23 / 32 with accuracy 71.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:  39%|███▉      | 85/217 [03:18<05:05,  2.31s/it, acc=0.531, loss=0.697]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 17 / 32 with accuracy 53.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:  40%|███▉      | 86/217 [03:20<05:02,  2.31s/it, acc=0.719, loss=0.645]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 23 / 32 with accuracy 71.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:  40%|████      | 87/217 [03:23<05:00,  2.31s/it, acc=0.656, loss=0.65] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 21 / 32 with accuracy 65.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:  41%|████      | 88/217 [03:25<04:57,  2.31s/it, acc=0.375, loss=0.722]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 12 / 32 with accuracy 37.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:  41%|████      | 89/217 [03:27<04:55,  2.30s/it, acc=0.688, loss=0.672]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 22 / 32 with accuracy 68.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:  41%|████▏     | 90/217 [03:30<04:52,  2.30s/it, acc=0.469, loss=0.72] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 15 / 32 with accuracy 46.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:  42%|████▏     | 91/217 [03:32<04:50,  2.30s/it, acc=0.656, loss=0.633]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 21 / 32 with accuracy 65.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:  42%|████▏     | 92/217 [03:34<04:47,  2.30s/it, acc=0.688, loss=0.673]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 22 / 32 with accuracy 68.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:  43%|████▎     | 93/217 [03:36<04:45,  2.30s/it, acc=0.531, loss=0.679]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 17 / 32 with accuracy 53.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:  43%|████▎     | 94/217 [03:39<04:43,  2.30s/it, acc=0.594, loss=0.682]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 19 / 32 with accuracy 59.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:  44%|████▍     | 95/217 [03:41<04:42,  2.31s/it, acc=0.562, loss=0.667]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 18 / 32 with accuracy 56.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:  45%|████▍     | 97/217 [03:46<04:42,  2.35s/it, acc=0.406, loss=0.715]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 13 / 32 with accuracy 40.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:  45%|████▌     | 98/217 [03:48<04:38,  2.34s/it, acc=0.5, loss=0.698]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 16 / 32 with accuracy 50.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=0/1:  46%|████▌     | 99/217 [03:50<04:35,  2.33s/it, acc=0.438, loss=0.708]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 14 / 32 with accuracy 43.75\n"
     ]
    }
   ],
   "source": [
    "model=finetune(1, dataloader, model, loss_fn, optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "has_gpu = torch.cuda.is_available()\n",
    "print(has_gpu)\n",
    "\n",
    "if torch.cuda.is_available(): \n",
    "    dev = \"cuda:0\" \n",
    "else: \n",
    "    dev = \"cpu\" \n",
    "device = torch.device(dev) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./datasets/borges_full.txt','r',encoding='utf8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import urllib.request  # the lib that handles the url stuff\n",
    "# target_url = 'https://ia601201.us.archive.org/2/items/BorgesObrasCompletasBorges/Borges-Obras-Completas-Borges_djvu.txt'\n",
    "# data = urllib.request.urlopen(target_url)\n",
    "# text = data.read().decode('utf-8')\n",
    "# with open('./datasets/borges_full.txt', 'w') as f:\n",
    "#     f.write(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quiero dejar escrita una confesión que a m tiempo será \n",
      "íntima y general  ya que las cosas que le ocurren a un \n",
      "hombre les ocurren a todos Estoy hablando de algo ya \n",
      "remoto y perdido los días de mi santo t los más antiguos \n",
      "Yo recibía los regalos y yo pensaba que no era más que \n",
      "un chico y que no había hecho nada  absolutamente nada \n",
      "para merecerlos  PoY supuesto nunca lo dije la nifiez es \n",
      "tímida  Desde entonces me has dado tantas cosas y son \n",
      "tantos los años y los recuerdos  Padre Norah los abuelos \n",
      "tu memoria y en ella la memoria de los mayores  los \n",
      "patios los esclavos el agúatele la carga de los húsares \n",
      "del Perú y el oprobio de Rosas   tu prisión valerosa \n",
      "cuando tantos hombres callábamos  las mañanas del Paso \n",
      "del Molino f de Ginebra y de Austin f las compartidas cla \n",
      "ridades  T sombras tu fresca ancianidad tu amor a Dv \n",
      "ckens y a Ea de Queiroz Madre  vos misma  \n",
      "\n",
      "Aquí estamos hablando los dos  et tout le resie est litié \n",
      "rature como escribió t con excelente literatura seríame \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "re.sub(\"[^a-zA-Z]+\", \"\", \"ABC12abc345def\")\n",
    "\n",
    "text = ''.join([i for i in text if i.isalpha() or i.isspace()])\n",
    "print(text[0:1000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "class Dataset(Dataset):\n",
    "        \n",
    "    def __init__(self,sequence_length,char_level):\n",
    "        \n",
    "\n",
    "        self.char_level = char_level\n",
    "        self.words = self.load_words()\n",
    "        self.uniq_words = self.get_uniq_words()\n",
    "\n",
    "        self.index_to_word = {index: word for index, word in enumerate(self.uniq_words)}\n",
    "        self.unk_word = 'UNK'\n",
    "        self.unk_word_index = len(self.index_to_word)+1\n",
    "        self.index_to_word[self.unk_word_index] = self.unk_word\n",
    "        self.word_to_index = {word: index for index, word in enumerate(self.uniq_words)}\n",
    "        self.sequence_length = sequence_length\n",
    "        self.words_indexes = [self.word_to_index[w] for w in self.words]\n",
    "        \n",
    "\n",
    "    def load_words(self):\n",
    "        with open('./datasets/borges.txt','r',encoding='utf8') as f:\n",
    "            text = f.read()        \n",
    "        if self.char_level==True:\n",
    "            return list(text)\n",
    "        else:\n",
    "            return text.split(' ')\n",
    "\n",
    "    def get_uniq_words(self):\n",
    "        word_counts = Counter(self.words)\n",
    "        return sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.words_indexes) - self.sequence_length\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return (\n",
    "            torch.tensor(self.words_indexes[index:index+self.sequence_length]),\n",
    "            torch.tensor(self.words_indexes[index+1:index+self.sequence_length+1]),\n",
    "        )    \n",
    "\n",
    "batch_size=10    \n",
    "sequence_length=100\n",
    "char_level = False\n",
    "    \n",
    "dataset = Dataset(sequence_length, char_level=char_level)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6090\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "words = dataset.load_words()\n",
    "len(set(words))\n",
    "\n",
    "print(dataset.unk_word_index)\n",
    "dataset.words_indexes.count(92)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TokenRNN(\n",
       "  (embedding): Embedding(6089, 128)\n",
       "  (lstm): LSTM(128, 512, num_layers=2, dropout=0.2, bidirectional=True)\n",
       "  (fc): Linear(in_features=1024, out_features=6089, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "dev = \"cuda:0\"\n",
    "\n",
    "class TokenRNN(nn.Module):\n",
    "    def __init__(self, dataset, use_gpu):\n",
    "        super(TokenRNN, self).__init__()\n",
    "        self.embedding_dim = 128\n",
    "        self.lstm_size = 512\n",
    "        self.num_layers = 2\n",
    "        self.bidirectional = True\n",
    "        self.use_gpu = use_gpu\n",
    "\n",
    "        n_vocab = len(dataset.uniq_words)\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=n_vocab,\n",
    "            embedding_dim=self.embedding_dim,\n",
    "        )\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.embedding_dim,\n",
    "            hidden_size=self.lstm_size,\n",
    "            num_layers=self.num_layers,\n",
    "            dropout=0.2,\n",
    "            bidirectional=self.bidirectional\n",
    "        )\n",
    "\n",
    "        bir=1\n",
    "        if self.bidirectional:\n",
    "            bir=2\n",
    "        self.fc = nn.Linear(self.lstm_size*bir, n_vocab)\n",
    "\n",
    "    def forward(self, x, prev_state):\n",
    "        if self.use_gpu:\n",
    "            x = x.to(device)\n",
    "        embed = self.embedding(x)\n",
    "        if self.use_gpu:\n",
    "            embed = embed.to(device)\n",
    "        output, state = self.lstm(embed, prev_state)\n",
    "        if self.use_gpu:\n",
    "            output = output.to(device)\n",
    "        logits = self.fc(output)\n",
    "\n",
    "        return logits, state\n",
    "\n",
    "    def init_state(self, sequence_length):\n",
    "        bir = 1\n",
    "        if self.bidirectional:\n",
    "            bir = 2\n",
    "        \n",
    "        h = torch.zeros(self.num_layers*bir, sequence_length, self.lstm_size)\n",
    "        if self.use_gpu:\n",
    "            h = h.to(device)\n",
    "        return (h,h)\n",
    "    \n",
    "    \n",
    "model = TokenRNN(dataset, True)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " doing model.to(device) True\n",
      "{'epoch': 0, 'batch': 0, 'loss': 8.713558197021484}\n",
      "{'epoch': 0, 'batch': 500, 'loss': 5.4991278648376465}\n",
      "{'epoch': 0, 'batch': 1000, 'loss': 5.258494853973389}\n",
      "{'epoch': 1, 'batch': 0, 'loss': 6.857874393463135}\n",
      "{'epoch': 1, 'batch': 500, 'loss': 3.3809866905212402}\n",
      "{'epoch': 1, 'batch': 1000, 'loss': 2.907339096069336}\n",
      "{'epoch': 2, 'batch': 0, 'loss': 5.977495193481445}\n",
      "{'epoch': 2, 'batch': 500, 'loss': 1.9419424533843994}\n",
      "{'epoch': 2, 'batch': 1000, 'loss': 1.5021312236785889}\n",
      "{'epoch': 3, 'batch': 0, 'loss': 4.880244731903076}\n",
      "{'epoch': 3, 'batch': 500, 'loss': 0.9647094011306763}\n",
      "{'epoch': 3, 'batch': 1000, 'loss': 0.800978422164917}\n",
      "{'epoch': 4, 'batch': 0, 'loss': 3.733320713043213}\n",
      "{'epoch': 4, 'batch': 500, 'loss': 0.5864612460136414}\n",
      "{'epoch': 4, 'batch': 1000, 'loss': 0.4179871380329132}\n",
      "{'epoch': 5, 'batch': 0, 'loss': 2.725355625152588}\n",
      "{'epoch': 5, 'batch': 500, 'loss': 0.3142109811306}\n",
      "{'epoch': 5, 'batch': 1000, 'loss': 0.23613715171813965}\n",
      "{'epoch': 6, 'batch': 0, 'loss': 1.5721194744110107}\n",
      "{'epoch': 6, 'batch': 500, 'loss': 0.19170020520687103}\n",
      "{'epoch': 6, 'batch': 1000, 'loss': 0.14019498229026794}\n",
      "{'epoch': 7, 'batch': 0, 'loss': 1.1198524236679077}\n",
      "{'epoch': 7, 'batch': 500, 'loss': 0.08959095925092697}\n",
      "{'epoch': 7, 'batch': 1000, 'loss': 0.057841427624225616}\n",
      "{'epoch': 8, 'batch': 0, 'loss': 0.6000018119812012}\n",
      "{'epoch': 8, 'batch': 500, 'loss': 0.029939090833067894}\n",
      "{'epoch': 8, 'batch': 1000, 'loss': 0.01919068582355976}\n",
      "{'epoch': 9, 'batch': 0, 'loss': 0.34161391854286194}\n",
      "{'epoch': 9, 'batch': 500, 'loss': 0.011728803627192974}\n",
      "{'epoch': 9, 'batch': 1000, 'loss': 0.010748126544058323}\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "\n",
    "epochs=10\n",
    "start = time.time()\n",
    "device = torch.device(device) \n",
    "\n",
    "def train(dataset, model):\n",
    "    print(f\" doing model.to(device) {model.use_gpu}\")\n",
    "\n",
    "    if model.use_gpu:\n",
    "        model.to(device)\n",
    "        \n",
    "    model.train()\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        state_h, state_c = model.init_state(sequence_length)\n",
    "        if model.use_gpu:\n",
    "            state_h = state_h.to(device)\n",
    "            state_c = state_c.to(device)\n",
    "\n",
    "        for batch, (x, y) in enumerate(dataloader):\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if model.use_gpu:\n",
    "                x.to(device)\n",
    "\n",
    "            y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
    "\n",
    "            if model.use_gpu:\n",
    "                y_pred = y_pred.to(device)\n",
    "                y = y.to(device)\n",
    "\n",
    "            loss = criterion(y_pred.transpose(1, 2), y)\n",
    "\n",
    "            state_h = state_h.detach()\n",
    "            state_c = state_c.detach()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch%500==0:\n",
    "                # print(time.time() - start)\n",
    "                # Be careful to overwrite our original name file!\n",
    "#                model_name = 'borges_second_pass.net'\n",
    "#                torch.save(model.state_dict(),model_name)\n",
    "                print({ 'epoch': epoch, 'batch': batch, 'loss': loss.item() })\n",
    "\n",
    "train(dataset, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "------\n",
    "\n",
    "## Saving the Model\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/saving_loading_models.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Be careful to overwrite our original name file!\n",
    "model_name = 'borges_second_pass.net'\n",
    "torch.save(model.state_dict(),model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TokenRNN(\n",
       "  (embedding): Embedding(6089, 128)\n",
       "  (lstm): LSTM(128, 512, num_layers=2, dropout=0.2, bidirectional=True)\n",
       "  (fc): Linear(in_features=1024, out_features=6089, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MUST MATCH THE EXACT SAME SETTINGS AS MODEL USED DURING TRAINING!\n",
    "model_name = 'borges_second_pass.net'\n",
    "\n",
    "model = TokenRNN(dataset, False)\n",
    "model.load_state_dict(torch.load(model_name))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.enabled = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "el universo vocales esquina indigno (|saias ya manuscrito para mi, calificar\n",
      "de esta vio en 1912 como como un detenido un dios se claros, de\n",
      "las zaguan de confianza y una casi una pie, una refutacién de la jefes la desdefaron. es blasfematorio®. es space\n",
      "\n",
      "hamiet, es blasfematorio®. y mortifica y ser el famoso el famoso el sentenciado diecinueve, fierro de shahrazad de piel de boletines...\n",
      "\n",
      "observé de vertiginosa de vertiginosa de dialéctica, de san\n",
      "lucas.\n",
      "\n",
      "estos regresa destino. y carpécrates; un remoto\n",
      "espejo asi sagrado a mera a refleja olviden que erratas, ver a voces a voces que se aterréd el tiempo de las palabras y de el\n"
     ]
    }
   ],
   "source": [
    "def predict(dataset, model, text, next_words=100, use_gpu=False):\n",
    "    model.eval()\n",
    "    \n",
    "    if use_gpu:\n",
    "        model.to(device)\n",
    "\n",
    "    words = text.split(' ')\n",
    "    state_h, state_c = model.init_state(len(words))\n",
    "    if use_gpu:\n",
    "        state_h = state_h.to(device)\n",
    "        state_c = state_c.to(device)\n",
    "\n",
    "    def get_index(word):\n",
    "        if word in dataset.word_to_index.keys():\n",
    "            return dataset.word_to_index[word]\n",
    "        else:\n",
    "            return dataset.unk_word_index\n",
    "    \n",
    "    for i in range(0, next_words):\n",
    "        \n",
    "        x = torch.tensor([[get_index(w) for w in words[i:]]])\n",
    "\n",
    "        y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
    "\n",
    "        last_word_logits = y_pred[0][-1]\n",
    "        last_word_logits = last_word_logits.to('cpu')\n",
    "\n",
    "        p = torch.nn.functional.softmax(last_word_logits, dim=0).detach().numpy()\n",
    "        word_index = np.random.choice(len(last_word_logits), p=p)\n",
    "        words.append(dataset.index_to_word[word_index])\n",
    "\n",
    "    return words\n",
    "\n",
    "import random\n",
    "r = random.randint(0, len(dataset.words))\n",
    "text = \"\".join(dataset.words[r:r+dataset.sequence_length])\n",
    "language_generated = predict(dataset, model, text=\"el universo\", next_words=100, use_gpu=False)\n",
    "\n",
    "print(' '.join(language_generated).lower())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p38",
   "language": "python",
   "name": "conda_pytorch_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
