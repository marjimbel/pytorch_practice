{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./datasets/borges.txt','r',encoding='utf8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El Aleph\n",
      "[Cuento. Texto completo]\n",
      "\n",
      "O God, | could be bounded in a nutshell\n",
      "and count myself a King of infinite space\n",
      "\n",
      "Hamiet, ll, 2\n",
      "\n",
      "But they will teach us that Eternity is the\n",
      "Standing still of the Present Time, a\n",
      "Nunc-stans (ast the Schools call it);\n",
      "\n",
      "which neither they, nor any else\n",
      "understand, no more than they would a\n",
      "Hic-stans for an Infinite greatnesse of\n",
      "Place.\n",
      "\n",
      "Leviathan, IV, 46\n",
      "\n",
      "La candente mafana de febrero en que Beatriz Viterbo muriéd, después de una\n",
      "imperiosa agonia que no se rebaj6 un solo instante ni al sentimentalismo ni al\n",
      "miedo, noté que las carteleras de fierro de la Plaza Constituci6n habian\n",
      "renovado no sé qué aviso de cigarrillos rubios; el hecho me doliéd, pues\n",
      "comprendi que el incesante y vasto universo ya se apartaba de ella y que ese\n",
      "cambio era el primero de una serie infinita. Cambiara el universo pero yo no,\n",
      "pensé con melancélica vanidad; alguna vez, lo sé, mi vana devocidén la habia\n",
      "exasperado; muerta yo podia consagrarme a su memoria, sin esperanza, pero\n",
      "t\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89845"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Entire Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_characters = set(text)\n",
    "len(all_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = dict(enumerate(all_characters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = {char: ind for ind,char in decoder.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text = np.array([encoder[char] for char in text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding\n",
    "\n",
    "As previously discussed, we need to one-hot encode our data inorder for it to work with the network structure. Make sure to review numpy if any of these operations confuse you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoder(encoded_text, num_uni_chars):\n",
    "    '''\n",
    "    encoded_text : batch of encoded text\n",
    "    \n",
    "    num_uni_chars = number of unique characters (len(set(text)))\n",
    "    '''\n",
    "    \n",
    "    # METHOD FROM:\n",
    "    # https://stackoverflow.com/questions/29831489/convert-encoded_textay-of-indices-to-1-hot-encoded-numpy-encoded_textay\n",
    "      \n",
    "    # Create a placeholder for zeros.\n",
    "    one_hot = np.zeros((encoded_text.size, num_uni_chars))\n",
    "    \n",
    "    # Convert data type for later use with pytorch (errors if we dont!)\n",
    "    one_hot = one_hot.astype(np.float32)\n",
    "\n",
    "    # Using fancy indexing fill in the 1s at the correct index locations\n",
    "    one_hot[np.arange(one_hot.shape[0]), encoded_text.flatten()] = 1.0\n",
    "    \n",
    "\n",
    "    # Reshape it so it matches the batch sahe\n",
    "    one_hot = one_hot.reshape((*encoded_text.shape, num_uni_chars))\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encoder(np.array([1,2,0]),3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "---------------\n",
    "# Creating Training Batches\n",
    "\n",
    "We need to create a function that will generate batches of characters along with the next character in the sequence as a label.\n",
    "\n",
    "-----------------\n",
    "------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 50)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_batches(encoded_text, samp_per_batch=10, seq_len=50):\n",
    "\n",
    "    char_per_batch = samp_per_batch * seq_len\n",
    "    num_batches_avail = int(len(encoded_text)/char_per_batch)\n",
    "    encoded_text = encoded_text[:num_batches_avail * char_per_batch]\n",
    "    encoded_text = encoded_text.reshape((samp_per_batch, -1))\n",
    "\n",
    "    def batching(encoded_text, seq_len):\n",
    "        # Go through each row in array.\n",
    "        for n in range(0, encoded_text.shape[1], seq_len):\n",
    "\n",
    "            # Grab feature characters\n",
    "            x = encoded_text[:, n:n+seq_len]\n",
    "\n",
    "            # y is the target shifted over by 1\n",
    "            y = np.zeros_like(x)\n",
    "\n",
    "            #\n",
    "            try:\n",
    "                y[:, :-1] = x[:, 1:]\n",
    "                y[:, -1]  = encoded_text[:, n+seq_len]\n",
    "\n",
    "            # FOR POTENTIAL INDEXING ERROR AT THE END\n",
    "            except:\n",
    "                y[:, :-1] = x[:, 1:]\n",
    "                y[:, -1] = encoded_text[:, 0]\n",
    "\n",
    "            yield x, y\n",
    "\n",
    "    return batching(encoded_text, seq_len)\n",
    "\n",
    "gen = generate_batches(encoded_text)\n",
    "x, y = next(iter(gen))\n",
    "x.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, all_chars, num_hidden=256, num_layers=4,drop_prob=0.5,use_gpu=False):\n",
    "        \n",
    "        \n",
    "        # SET UP ATTRIBUTES\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.num_layers = num_layers\n",
    "        self.num_hidden = num_hidden\n",
    "        self.use_gpu = use_gpu\n",
    "        \n",
    "        #CHARACTER SET, ENCODER, and DECODER\n",
    "        self.all_chars = all_chars\n",
    "        self.decoder = dict(enumerate(all_chars))\n",
    "        self.encoder = {char: ind for ind,char in decoder.items()}\n",
    "        \n",
    "        \n",
    "        self.lstm = nn.LSTM(len(self.all_chars), num_hidden, num_layers, dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        self.fc_linear = nn.Linear(num_hidden, len(self.all_chars))\n",
    "      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "                  \n",
    "        \n",
    "        lstm_output, hidden = self.lstm(x, hidden)\n",
    "        \n",
    "        \n",
    "        drop_output = self.dropout(lstm_output)\n",
    "        \n",
    "        drop_output = drop_output.contiguous().view(-1, self.num_hidden)\n",
    "        \n",
    "        \n",
    "        final_out = self.fc_linear(drop_output)\n",
    "        \n",
    "        \n",
    "        return final_out, hidden\n",
    "    \n",
    "    \n",
    "    def hidden_state(self, batch_size):\n",
    "        '''\n",
    "        Used as separate method to account for both GPU and CPU users.\n",
    "        '''\n",
    "        \n",
    "        if self.use_gpu:\n",
    "            \n",
    "            hidden = (torch.zeros(self.num_layers,batch_size,self.num_hidden).to(\"mps\"),\n",
    "                     torch.zeros(self.num_layers,batch_size,self.num_hidden).to(\"mps\"))\n",
    "        else:\n",
    "            hidden = (torch.zeros(self.num_layers,batch_size,self.num_hidden),\n",
    "                     torch.zeros(self.num_layers,batch_size,self.num_hidden))\n",
    "        \n",
    "        return hidden\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data and Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_percent = 0.1\n",
    "train_ind = int(len(encoded_text) * (train_percent))\n",
    "train_data = encoded_text[:train_ind]\n",
    "val_data = encoded_text[train_ind:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " size of generator 89\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8984,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## HyperParams\n",
    "epochs = 20\n",
    "batch_size = 1\n",
    "seq_len = 100\n",
    "tracker = 10\n",
    "num_lstm_layers = 3\n",
    "dropout_p = .03\n",
    "hidden_dim = 256\n",
    "num_char = max(encoded_text)+1\n",
    "\n",
    "model = CharModel(\n",
    "    all_chars=all_characters,\n",
    "    num_hidden=hidden_dim,\n",
    "    num_layers=num_lstm_layers,\n",
    "    drop_prob=dropout_p,\n",
    "    use_gpu=False,\n",
    ")\n",
    "\n",
    "\n",
    "data_iter = generate_batches(train_data,batch_size,seq_len)\n",
    "print(f\" size of generator {sum(1 for _ in data_iter)}\")\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Step: 25 Val Loss: 3.119316816329956\n",
      "Epoch: 0 Step: 50 Val Loss: 3.0813751220703125\n",
      "Epoch: 0 Step: 75 Val Loss: 3.027113676071167\n",
      "Epoch: 1 Step: 100 Val Loss: 3.060596227645874\n",
      "Epoch: 1 Step: 125 Val Loss: 3.05379056930542\n",
      "Epoch: 1 Step: 150 Val Loss: 3.052138328552246\n",
      "Epoch: 1 Step: 175 Val Loss: 3.0348711013793945\n",
      "Epoch: 2 Step: 200 Val Loss: 3.0866332054138184\n",
      "Epoch: 2 Step: 225 Val Loss: 3.07075834274292\n",
      "Epoch: 2 Step: 250 Val Loss: 3.0325026512145996\n",
      "Epoch: 2 Step: 275 Val Loss: 3.054020881652832\n",
      "Epoch: 3 Step: 300 Val Loss: 3.054258108139038\n",
      "Epoch: 3 Step: 325 Val Loss: 3.061342239379883\n",
      "Epoch: 3 Step: 350 Val Loss: 3.0154459476470947\n",
      "Epoch: 4 Step: 375 Val Loss: 3.0664584636688232\n",
      "Epoch: 4 Step: 400 Val Loss: 2.9404304027557373\n",
      "Epoch: 4 Step: 425 Val Loss: 2.8671436309814453\n",
      "Epoch: 4 Step: 450 Val Loss: 2.9249086380004883\n",
      "Epoch: 5 Step: 475 Val Loss: 2.780125617980957\n",
      "Epoch: 5 Step: 500 Val Loss: 2.8080756664276123\n",
      "Epoch: 5 Step: 525 Val Loss: 2.7295358180999756\n",
      "Epoch: 6 Step: 550 Val Loss: 2.74923038482666\n",
      "Epoch: 6 Step: 575 Val Loss: 2.7159223556518555\n",
      "Epoch: 6 Step: 600 Val Loss: 2.6527373790740967\n",
      "Epoch: 6 Step: 625 Val Loss: 2.649460792541504\n",
      "Epoch: 7 Step: 650 Val Loss: 2.661949396133423\n",
      "Epoch: 7 Step: 675 Val Loss: 2.5998008251190186\n",
      "Epoch: 7 Step: 700 Val Loss: 2.551443576812744\n",
      "Epoch: 8 Step: 725 Val Loss: 2.5785927772521973\n",
      "Epoch: 8 Step: 750 Val Loss: 2.546696901321411\n",
      "Epoch: 8 Step: 775 Val Loss: 2.563145637512207\n",
      "Epoch: 8 Step: 800 Val Loss: 2.540846586227417\n",
      "Epoch: 9 Step: 825 Val Loss: 2.5859251022338867\n",
      "Epoch: 9 Step: 850 Val Loss: 2.485318899154663\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model.train()\n",
    "\n",
    "# Check to see if using GPU\n",
    "\n",
    "if model.use_gpu:\n",
    "    torch.device(\"mps\")\n",
    "    model.to(\"mps\")\n",
    "\n",
    "for i in range(epochs):\n",
    "    \n",
    "    hidden = model.hidden_state(batch_size)\n",
    "    \n",
    "    for x,y in generate_batches(train_data,batch_size,seq_len):\n",
    "        \n",
    "        tracker += 1\n",
    "        x = one_hot_encoder(x,num_char)\n",
    "        inputs = torch.from_numpy(x)\n",
    "        targets = torch.from_numpy(y)\n",
    "        if model.use_gpu:\n",
    "            inputs = inputs.to(\"mps\")\n",
    "            targets = targets.to(\"mps\")\n",
    "            \n",
    "        # If we don't reset we would backpropagate through all training history\n",
    "        hidden = tuple([state.data for state in hidden])\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        lstm_output, hidden = model.forward(inputs,hidden)\n",
    "        loss = criterion(lstm_output,targets.view(batch_size*seq_len).long())\n",
    "        loss.backward()\n",
    "        \n",
    "        # LET\"S CLIP JUST IN CASE\n",
    "        nn.utils.clip_grad_norm_(model.parameters(),max_norm=5)\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "        ###################################\n",
    "        ### CHECK ON VALIDATION SET ######\n",
    "        #################################\n",
    "        \n",
    "        if tracker % 25 == 0:\n",
    "            \n",
    "            val_hidden = model.hidden_state(batch_size)\n",
    "            val_losses = []\n",
    "            model.eval()\n",
    "            \n",
    "            for x,y in generate_batches(val_data,batch_size,seq_len):\n",
    "                \n",
    "                x = one_hot_encoder(x,num_char)\n",
    "                inputs = torch.from_numpy(x)\n",
    "                targets = torch.from_numpy(y)\n",
    "                if model.use_gpu:\n",
    "                    inputs = inputs.to(\"mps\")\n",
    "                    targets = targets.to(\"mps\")\n",
    "                    \n",
    "                val_hidden = tuple([state.data for state in val_hidden])\n",
    "                lstm_output, val_hidden = model.forward(inputs,val_hidden)\n",
    "                val_loss = criterion(lstm_output,targets.view(batch_size*seq_len).long())\n",
    "                val_losses.append(val_loss.item())\n",
    "            \n",
    "            # Reset to training model after val for loop\n",
    "            model.train()\n",
    "            \n",
    "            print(f\"Epoch: {i} Step: {tracker} Val Loss: {val_loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "------\n",
    "\n",
    "## Saving the Model\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/saving_loading_models.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Be careful to overwrite our original name file!\n",
    "model_name = 'borges_first_pass.net'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MUST MATCH THE EXACT SAME SETTINGS AS MODEL USED DURING TRAINING!\n",
    "\n",
    "model = CharModel(\n",
    "    all_chars=all_characters,\n",
    "    num_hidden=hidden_dim,\n",
    "    num_layers=num_lstm_layers,\n",
    "    drop_prob=dropout_p,\n",
    "    use_gpu=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CharModel(\n",
       "  (lstm): LSTM(91, 256, num_layers=3, batch_first=True, dropout=0.03)\n",
       "  (dropout): Dropout(p=0.03, inplace=False)\n",
       "  (fc_linear): Linear(in_features=256, out_features=91, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(model_name))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_char(model, char, hidden=None, k=1):\n",
    "        \n",
    "        encoded_text = model.encoder[char]\n",
    "        encoded_text = np.array([[encoded_text]])\n",
    "        encoded_text = one_hot_encoder(encoded_text, len(model.all_chars))\n",
    "        inputs = torch.from_numpy(encoded_text)\n",
    "        \n",
    "        if(model.use_gpu):\n",
    "            inputs = inputs.to(\"mps\")\n",
    "        \n",
    "\n",
    "        hidden = tuple([state.data for state in hidden])\n",
    "        lstm_out, hidden = model(inputs, hidden)\n",
    "        probs = F.softmax(lstm_out, dim=1).data\n",
    "        \n",
    "        if(model.use_gpu):\n",
    "            probs = probs.to(\"mps\")\n",
    "        \n",
    "\n",
    "        probs, index_positions = probs.topk(k)\n",
    "        index_positions = index_positions.numpy().squeeze()\n",
    "        \n",
    "        probs = probs.numpy().flatten()\n",
    "        probs = probs/probs.sum()\n",
    "        \n",
    "        # randomly choose a character based on probabilities\n",
    "        char = np.random.choice(index_positions, p=probs)\n",
    "       \n",
    "        # return the encoded value of the predicted char and the hidden state\n",
    "        return model.decoder[char], hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, size, seed='The', k=1):\n",
    "        \n",
    "    if(model.use_gpu):\n",
    "        model.to(\"mps\")\n",
    "    else:\n",
    "        model.cpu()\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    output_chars = [c for c in seed]\n",
    "\n",
    "    hidden = model.hidden_state(1)\n",
    "    for char in seed:\n",
    "        char, hidden = predict_next_char(model, char, hidden, k=k)\n",
    "    \n",
    "\n",
    "    output_chars.append(char)\n",
    "\n",
    "    for i in range(size):\n",
    "        \n",
    "        # predict based off very last letter in output_chars\n",
    "        char, hidden = predict_next_char(model, output_chars[-1], hidden, k=k)\n",
    "        \n",
    "        # add predicted character\n",
    "        output_chars.append(char)\n",
    "    \n",
    "    # return string of predicted text\n",
    "    return ''.join(output_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The un los de en de entesteria distera de lara leren es des de el de lo les el elas de le dera en de de en des de lo el le de les lo enteria la el la le el dian an de da lerasa en el de lostos lo los de de lerio de la elterasa des le da el entaria el ena dienta le la es las le ente les de des la elte en la le la lo elas de es de de el derio dista les enta los es lesa le entas de es de en le eltaro de le dia lariana le lo elte lo de lerena les enta le le le ena de de les es de dera de la le de de en de lerento de da elte la des le de da de dertoro en ena ena estas lesas el dertes da dertosta da le le el es lere de las la de de ela lo le les en le laresa le de le la la de el los derta ena de le la el esto des des de el de en de de dicen le en de de el esto le lo des la de le dicio le lo les entara en en de dis las de de la des de es de el das dica de le ente de ela le de le da eltes el los de les las de de la le la la el le le de eltara des\n",
      "de les das elta lorante le esa lo de ena esta el de \n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, 1000, seed='The ', k=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p38",
   "language": "python",
   "name": "conda_pytorch_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
