{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./datasets/borges_full.txt','r',encoding='utf8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "JORGE LUIS BORGES \n",
      "\n",
      "\n",
      "1929: Segundo Premio Municipal de Li- \n",
      "teratura. \n",
      "\n",
      "1944: Gran Premio de Honor de ia So- \n",
      "ciedad Argentina de Escritores. \n",
      "\n",
      "1949: Miembro de la Academia Goethea- \n",
      "na de San Pablo, Brasil. \n",
      "\n",
      "1950: Presidente de la Sociedad Argen- \n",
      "tina de Escritores (hasta 1953), \n",
      "\n",
      "1955: Director de i a Biblioteca Na- \n",
      "cional (hasta 1973). \n",
      "\n",
      "Miembro de número de la Academia \n",
      "Argentina de Letras, \n",
      "\n",
      "Director del Instituto de Literatura \n",
      "í Alemana de la Facultad de Filosofía y \n",
      "! Letras de la Universidad de Buenos \n",
      "Aires. \n",
      "\n",
      "1956: Primer Premio Nacional de Lite- \n",
      "ratura. \n",
      "\n",
      "Doctor honorís causa de ia Universi- \n",
      "dad de Cuyo (Mendoza), \n",
      "\n",
      "Profesor titular de Literatura Inglesa \n",
      "y Norteamericana de la Facultad de Fi- \n",
      "losofía y Letras de la Universidad de \n",
      "Buenos Aires, \n",
      "\n",
      "i \n",
      "\n",
      "1961: Premio Internacional de Literatu- \n",
      "ra Formentor, Mallorca. \n",
      "\n",
      "Commsndatore del Gobierno de Ita- \n",
      "lia. \n",
      "\n",
      "1962: Commandeur da 1‘Ordre des \n",
      "L&ttres et des Arta del Gobierno de \n",
      "Francia. \n",
      "\n",
      "1963: Gran Premio del Fondo \n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_characters = set(text)\n",
    "len(all_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = dict(enumerate(all_characters))\n",
    "encoder = {char: ind for ind,char in decoder.items()}\n",
    "encoded_text = np.array([encoder[char] for char in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoder(encoded_text, num_uni_chars):\n",
    "    '''\n",
    "    encoded_text : batch of encoded text\n",
    "    \n",
    "    num_uni_chars = number of unique characters (len(set(text)))\n",
    "    '''\n",
    "    \n",
    "    # METHOD FROM:\n",
    "    # https://stackoverflow.com/questions/29831489/convert-encoded_textay-of-indices-to-1-hot-encoded-numpy-encoded_textay\n",
    "      \n",
    "    # Create a placeholder for zeros.\n",
    "    one_hot = np.zeros((encoded_text.size, num_uni_chars))\n",
    "    \n",
    "    # Convert data type for later use with pytorch (errors if we dont!)\n",
    "    one_hot = one_hot.astype(np.float32)\n",
    "\n",
    "    # Using fancy indexing fill in the 1s at the correct index locations\n",
    "    one_hot[np.arange(one_hot.shape[0]), encoded_text.flatten()] = 1.0\n",
    "    \n",
    "\n",
    "    # Reshape it so it matches the batch sahe\n",
    "    one_hot = one_hot.reshape((*encoded_text.shape, num_uni_chars))\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 50)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_batches(encoded_text, samp_per_batch=10, seq_len=50):\n",
    "\n",
    "    char_per_batch = samp_per_batch * seq_len\n",
    "    num_batches_avail = int(len(encoded_text)/char_per_batch)\n",
    "    encoded_text = encoded_text[:num_batches_avail * char_per_batch]\n",
    "    encoded_text = encoded_text.reshape((samp_per_batch, -1))\n",
    "\n",
    "    def batching(encoded_text, seq_len):\n",
    "        # Go through each row in array.\n",
    "        for n in range(0, encoded_text.shape[1], seq_len):\n",
    "\n",
    "            # Grab feature characters\n",
    "            x = encoded_text[:, n:n+seq_len]\n",
    "\n",
    "            # y is the target shifted over by 1\n",
    "            y = np.zeros_like(x)\n",
    "\n",
    "            #\n",
    "            try:\n",
    "                y[:, :-1] = x[:, 1:]\n",
    "                y[:, -1]  = encoded_text[:, n+seq_len]\n",
    "\n",
    "            # FOR POTENTIAL INDEXING ERROR AT THE END\n",
    "            except:\n",
    "                y[:, :-1] = x[:, 1:]\n",
    "                y[:, -1] = encoded_text[:, 0]\n",
    "\n",
    "            yield x, y\n",
    "\n",
    "    return batching(encoded_text, seq_len)\n",
    "\n",
    "gen = generate_batches(encoded_text)\n",
    "x, y = next(iter(gen))\n",
    "x.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, all_chars, num_hidden=256, num_layers=4,drop_prob=0.5,use_gpu=False):\n",
    "        \n",
    "        \n",
    "        # SET UP ATTRIBUTES\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.num_layers = num_layers\n",
    "        self.num_hidden = num_hidden\n",
    "        self.use_gpu = use_gpu\n",
    "        \n",
    "        #CHARACTER SET, ENCODER, and DECODER\n",
    "        self.all_chars = all_chars\n",
    "        self.decoder = dict(enumerate(all_chars))\n",
    "        self.encoder = {char: ind for ind,char in decoder.items()}\n",
    "        \n",
    "        \n",
    "        self.lstm = nn.LSTM(len(self.all_chars), num_hidden, num_layers, dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        self.fc_linear = nn.Linear(num_hidden, len(self.all_chars))\n",
    "      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "                  \n",
    "        \n",
    "        lstm_output, hidden = self.lstm(x, hidden)\n",
    "        \n",
    "        \n",
    "        drop_output = self.dropout(lstm_output)\n",
    "        \n",
    "        drop_output = drop_output.contiguous().view(-1, self.num_hidden)\n",
    "        \n",
    "        \n",
    "        final_out = self.fc_linear(drop_output)\n",
    "        \n",
    "        \n",
    "        return final_out, hidden\n",
    "    \n",
    "    \n",
    "    def hidden_state(self, batch_size):\n",
    "        '''\n",
    "        Used as separate method to account for both GPU and CPU users.\n",
    "        '''\n",
    "        \n",
    "        if self.use_gpu:\n",
    "            \n",
    "            hidden = (torch.zeros(self.num_layers,batch_size,self.num_hidden).to(\"mps\"),\n",
    "                     torch.zeros(self.num_layers,batch_size,self.num_hidden).to(\"mps\"))\n",
    "        else:\n",
    "            hidden = (torch.zeros(self.num_layers,batch_size,self.num_hidden),\n",
    "                     torch.zeros(self.num_layers,batch_size,self.num_hidden))\n",
    "        \n",
    "        return hidden\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data and Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_percent = 0.1\n",
    "train_ind = int(len(encoded_text) * (train_percent))\n",
    "train_data = encoded_text[:train_ind]\n",
    "val_data = encoded_text[train_ind:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " size of generator 2045\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(204531,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## HyperParams\n",
    "epochs = 20\n",
    "batch_size = 1\n",
    "seq_len = 100\n",
    "tracker = 10\n",
    "num_lstm_layers = 3\n",
    "dropout_p = .03\n",
    "hidden_dim = 256\n",
    "num_char = max(encoded_text)+1\n",
    "\n",
    "model = CharModel(\n",
    "    all_chars=all_characters,\n",
    "    num_hidden=hidden_dim,\n",
    "    num_layers=num_lstm_layers,\n",
    "    drop_prob=dropout_p,\n",
    "    use_gpu=False,\n",
    ")\n",
    "\n",
    "\n",
    "data_iter = generate_batches(train_data,batch_size,seq_len)\n",
    "print(f\" size of generator {sum(1 for _ in data_iter)}\")\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model.train()\n",
    "\n",
    "# Check to see if using GPU\n",
    "\n",
    "if model.use_gpu:\n",
    "    torch.device(\"mps\")\n",
    "    model.to(\"mps\")\n",
    "\n",
    "for i in range(epochs):\n",
    "    \n",
    "    hidden = model.hidden_state(batch_size)\n",
    "    \n",
    "    for x,y in generate_batches(train_data,batch_size,seq_len):\n",
    "        \n",
    "        tracker += 1\n",
    "        x = one_hot_encoder(x,num_char)\n",
    "        inputs = torch.from_numpy(x)\n",
    "        targets = torch.from_numpy(y)\n",
    "        if model.use_gpu:\n",
    "            inputs = inputs.to(\"mps\")\n",
    "            targets = targets.to(\"mps\")\n",
    "            \n",
    "        # If we don't reset we would backpropagate through all training history\n",
    "        hidden = tuple([state.data for state in hidden])\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        lstm_output, hidden = model.forward(inputs,hidden)\n",
    "        loss = criterion(lstm_output,targets.view(batch_size*seq_len).long())\n",
    "        loss.backward()\n",
    "        \n",
    "        # LET\"S CLIP JUST IN CASE\n",
    "        nn.utils.clip_grad_norm_(model.parameters(),max_norm=5)\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "        ###################################\n",
    "        ### CHECK ON VALIDATION SET ######\n",
    "        #################################\n",
    "        \n",
    "        if tracker % 25 == 0:\n",
    "            \n",
    "            val_hidden = model.hidden_state(batch_size)\n",
    "            val_losses = []\n",
    "            model.eval()\n",
    "            \n",
    "            for x,y in generate_batches(val_data,batch_size,seq_len):\n",
    "                \n",
    "                x = one_hot_encoder(x,num_char)\n",
    "                inputs = torch.from_numpy(x)\n",
    "                targets = torch.from_numpy(y)\n",
    "                if model.use_gpu:\n",
    "                    inputs = inputs.to(\"mps\")\n",
    "                    targets = targets.to(\"mps\")\n",
    "                    \n",
    "                val_hidden = tuple([state.data for state in val_hidden])\n",
    "                lstm_output, val_hidden = model.forward(inputs,val_hidden)\n",
    "                val_loss = criterion(lstm_output,targets.view(batch_size*seq_len).long())\n",
    "                val_losses.append(val_loss.item())\n",
    "            \n",
    "            # Reset to training model after val for loop\n",
    "            model.train()\n",
    "            \n",
    "            print(f\"Epoch: {i} Step: {tracker} Val Loss: {val_loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "------\n",
    "\n",
    "## Saving the Model\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/saving_loading_models.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Be careful to overwrite our original name file!\n",
    "model_name = 'borges_first_pass.net'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MUST MATCH THE EXACT SAME SETTINGS AS MODEL USED DURING TRAINING!\n",
    "\n",
    "model = CharModel(\n",
    "    all_chars=all_characters,\n",
    "    num_hidden=hidden_dim,\n",
    "    num_layers=num_lstm_layers,\n",
    "    drop_prob=dropout_p,\n",
    "    use_gpu=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CharModel(\n",
       "  (lstm): LSTM(91, 256, num_layers=3, batch_first=True, dropout=0.03)\n",
       "  (dropout): Dropout(p=0.03, inplace=False)\n",
       "  (fc_linear): Linear(in_features=256, out_features=91, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(model_name))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_char(model, char, hidden=None, k=1):\n",
    "        \n",
    "        encoded_text = model.encoder[char]\n",
    "        encoded_text = np.array([[encoded_text]])\n",
    "        encoded_text = one_hot_encoder(encoded_text, len(model.all_chars))\n",
    "        inputs = torch.from_numpy(encoded_text)\n",
    "        \n",
    "        if(model.use_gpu):\n",
    "            inputs = inputs.to(\"mps\")\n",
    "        \n",
    "\n",
    "        hidden = tuple([state.data for state in hidden])\n",
    "        lstm_out, hidden = model(inputs, hidden)\n",
    "        probs = F.softmax(lstm_out, dim=1).data\n",
    "        \n",
    "        if(model.use_gpu):\n",
    "            probs = probs.to(\"mps\")\n",
    "        \n",
    "\n",
    "        probs, index_positions = probs.topk(k)\n",
    "        index_positions = index_positions.numpy().squeeze()\n",
    "        \n",
    "        probs = probs.numpy().flatten()\n",
    "        probs = probs/probs.sum()\n",
    "        \n",
    "        # randomly choose a character based on probabilities\n",
    "        char = np.random.choice(index_positions, p=probs)\n",
    "       \n",
    "        # return the encoded value of the predicted char and the hidden state\n",
    "        return model.decoder[char], hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, size, seed='The', k=1):\n",
    "        \n",
    "    if(model.use_gpu):\n",
    "        model.to(\"mps\")\n",
    "    else:\n",
    "        model.cpu()\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    output_chars = [c for c in seed]\n",
    "\n",
    "    hidden = model.hidden_state(1)\n",
    "    for char in seed:\n",
    "        char, hidden = predict_next_char(model, char, hidden, k=k)\n",
    "    \n",
    "\n",
    "    output_chars.append(char)\n",
    "\n",
    "    for i in range(size):\n",
    "        \n",
    "        # predict based off very last letter in output_chars\n",
    "        char, hidden = predict_next_char(model, output_chars[-1], hidden, k=k)\n",
    "        \n",
    "        # add predicted character\n",
    "        output_chars.append(char)\n",
    "    \n",
    "    # return string of predicted text\n",
    "    return ''.join(output_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El universo del poenio de la praceria de presiontas enta laseras en pranatina des pora en\n",
      "tomesas cristertan y las pracion de la raatianta en en tontorao de las dencara de la riata de la poesa a las calte de poraateros consas era ento de parecaro el poena de la reretara de las porcera asa el esa el estrota la paesaros eltoraas del precero estas arastorades ena\n",
      "ses cara\n",
      "ento listoracan en en poera de preracras de poraas de los paena antorios que la porata el tranataras de presas des pricena la laciateria de la porrecana lan presara, de lo priciés de paraa una lan esto el pareces de portaco de las decarios del\n",
      "dentician el eltariantes del prinaces de palosonte los tronatasas de caratarianan en estorio del\n",
      "estasas, esterante de lo rastas de las deciante de la\n",
      "\n",
      "uasa esta del palacian del crentino en esitaraa en elto lertentara al entarera de las poratas del pracecoros crensorta des prantera de los decas erta de la raasa el lo sastentaro del lacatoraca las pritira an esitrita del caratario en eposas de \n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, 1000, seed='El universo ', k=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
