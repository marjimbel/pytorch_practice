{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./datasets/borges_full.txt','r',encoding='utf8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import urllib.request  # the lib that handles the url stuff\n",
    "# target_url = 'https://ia601201.us.archive.org/2/items/BorgesObrasCompletasBorges/Borges-Obras-Completas-Borges_djvu.txt'\n",
    "# data = urllib.request.urlopen(target_url)\n",
    "# text = data.read().decode('utf-8')\n",
    "# with open('./datasets/borges_full.txt', 'w') as f:\n",
    "#     f.write(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "JORGE LUIS BORGES \n",
      "\n",
      "\n",
      "1929: Segundo Premio Municipal de Li- \n",
      "teratura. \n",
      "\n",
      "1944: Gran Premio de Honor de ia So- \n",
      "ciedad Argentina de Escritores. \n",
      "\n",
      "1949: Miembro de la Academia Goethea- \n",
      "na de San Pablo, Brasil. \n",
      "\n",
      "1950: Presidente de la Sociedad Argen- \n",
      "tina de Escritores (hasta 1953), \n",
      "\n",
      "1955: Director de i a Biblioteca Na- \n",
      "cional (hasta 1973). \n",
      "\n",
      "Miembro de número de la Academia \n",
      "Argentina de Letras, \n",
      "\n",
      "Director del Instituto de Literatura \n",
      "í Alemana de la Facultad de Filosofía y \n",
      "! Letras de la Universidad de Buenos \n",
      "Aires. \n",
      "\n",
      "1956: Primer Premio Nacional de Lite- \n",
      "ratura. \n",
      "\n",
      "Doctor honorís causa de ia Universi- \n",
      "dad de Cuyo (Mendoza), \n",
      "\n",
      "Profesor titular de Literatura Inglesa \n",
      "y Norteamericana de la Facultad de Fi- \n",
      "losofía y Letras de la Universidad de \n",
      "Buenos Aires, \n",
      "\n",
      "i \n",
      "\n",
      "1961: Premio Internacional de Literatu- \n",
      "ra Formentor, Mallorca. \n",
      "\n",
      "Commsndatore del Gobierno de Ita- \n",
      "lia. \n",
      "\n",
      "1962: Commandeur da 1‘Ordre des \n",
      "L&ttres et des Arta del Gobierno de \n",
      "Francia. \n",
      "\n",
      "1963: Gran Premio del Fondo \n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2045312\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "131"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = 'pepe'    \n",
    "tt = list(text)\n",
    "print(len(tt))\n",
    "len(set(tt))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "class Dataset(Dataset):\n",
    "        \n",
    "    def __init__(self,sequence_length,char_level):\n",
    "        \n",
    "\n",
    "        self.char_level = char_level\n",
    "        self.words = self.load_words()\n",
    "        self.uniq_words = self.get_uniq_words()\n",
    "\n",
    "        self.index_to_word = {index+1: word for index, word in enumerate(self.uniq_words)}\n",
    "        self.unk_word = 'UNK'\n",
    "        self.unk_word_index = 0 \n",
    "        self.index_to_word[self.unk_word_index] = self.unk_word\n",
    "        self.word_to_index = {word: index for index, word in enumerate(self.uniq_words)}\n",
    "        self.sequence_length = sequence_length\n",
    "        self.words_indexes = [self.word_to_index[w] for w in self.words]\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "    def load_words(self):\n",
    "        with open('./datasets/borges.txt','r',encoding='utf8') as f:\n",
    "            text = f.read()        \n",
    "        if self.char_level>1:\n",
    "            return list(text)\n",
    "        else:\n",
    "            return text.split(' ')\n",
    "\n",
    "    def get_uniq_words(self):\n",
    "        word_counts = Counter(self.words)\n",
    "        return sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.words_indexes) - self.sequence_length\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return (\n",
    "            torch.tensor(self.words_indexes[index:index+self.sequence_length]),\n",
    "            torch.tensor(self.words_indexes[index+1:index+self.sequence_length+1]),\n",
    "        )    \n",
    "\n",
    "batch_size=10    \n",
    "sequence_length=50\n",
    "char_level = True\n",
    "    \n",
    "dataset = Dataset(sequence_length, char_level=char_level)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TokenRNN(\n",
       "  (embedding): Embedding(6089, 128)\n",
       "  (lstm): LSTM(128, 256, num_layers=2, dropout=0.2, bidirectional=True)\n",
       "  (fc): Linear(in_features=512, out_features=6089, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class TokenRNN(nn.Module):\n",
    "    def __init__(self, dataset):\n",
    "        super(TokenRNN, self).__init__()\n",
    "        self.embedding_dim = 128\n",
    "        self.lstm_size = 256\n",
    "        self.num_layers = 2\n",
    "        self.bidirectional = True\n",
    "\n",
    "        n_vocab = len(dataset.uniq_words)\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=n_vocab,\n",
    "            embedding_dim=self.embedding_dim,\n",
    "        )\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.embedding_dim,\n",
    "            hidden_size=self.lstm_size,\n",
    "            num_layers=self.num_layers,\n",
    "            dropout=0.2,\n",
    "            bidirectional=self.bidirectional\n",
    "        )\n",
    "\n",
    "        bir=1\n",
    "        if self.bidirectional:\n",
    "            bir=2\n",
    "        self.fc = nn.Linear(self.lstm_size*bir, n_vocab)\n",
    "\n",
    "    def forward(self, x, prev_state):\n",
    "        embed = self.embedding(x)\n",
    "        output, state = self.lstm(embed, prev_state)\n",
    "        logits = self.fc(output)\n",
    "        return logits, state\n",
    "\n",
    "    def init_state(self, sequence_length):\n",
    "        bir = 1\n",
    "        if self.bidirectional:\n",
    "            bir = 2\n",
    "            \n",
    "        return (torch.zeros(self.num_layers*bir, sequence_length, self.lstm_size),\n",
    "                torch.zeros(self.num_layers*bir, sequence_length, self.lstm_size))\n",
    "    \n",
    "    \n",
    "model = TokenRNN(dataset)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.32216215133667\n",
      "{'epoch': 0, 'batch': 60, 'loss': 7.30534029006958}\n",
      "7.277010202407837\n",
      "{'epoch': 0, 'batch': 120, 'loss': 7.602330207824707}\n",
      "16.376984119415283\n",
      "{'epoch': 0, 'batch': 240, 'loss': 7.081428050994873}\n",
      "22.630858898162842\n",
      "{'epoch': 0, 'batch': 330, 'loss': 6.060498237609863}\n",
      "27.728796005249023\n",
      "{'epoch': 0, 'batch': 420, 'loss': 6.613697052001953}\n",
      "30.57123303413391\n",
      "{'epoch': 0, 'batch': 450, 'loss': 6.12847900390625}\n",
      "34.46856498718262\n",
      "{'epoch': 0, 'batch': 510, 'loss': 5.417666912078857}\n",
      "41.7786762714386\n",
      "{'epoch': 0, 'batch': 600, 'loss': 6.3568220138549805}\n",
      "56.40977716445923\n",
      "{'epoch': 0, 'batch': 840, 'loss': 5.321784019470215}\n",
      "60.75643515586853\n",
      "{'epoch': 0, 'batch': 900, 'loss': 6.187918663024902}\n",
      "65.06380200386047\n",
      "{'epoch': 0, 'batch': 960, 'loss': 5.5062360763549805}\n",
      "78.73007988929749\n",
      "{'epoch': 0, 'batch': 1170, 'loss': 5.486883640289307}\n",
      "80.43812394142151\n",
      "{'epoch': 0, 'batch': 1200, 'loss': 5.677637100219727}\n",
      "82.55122399330139\n",
      "{'epoch': 0, 'batch': 1230, 'loss': 5.7039079666137695}\n",
      "84.89132618904114\n",
      "{'epoch': 0, 'batch': 1260, 'loss': 5.192437171936035}\n",
      "103.58062195777893\n",
      "{'epoch': 1, 'batch': 150, 'loss': 4.599648475646973}\n",
      "112.62561392784119\n",
      "{'epoch': 1, 'batch': 300, 'loss': 5.620144367218018}\n",
      "114.34151315689087\n",
      "{'epoch': 1, 'batch': 330, 'loss': 4.040642738342285}\n",
      "116.04664206504822\n",
      "{'epoch': 1, 'batch': 360, 'loss': 4.639875888824463}\n",
      "124.10553526878357\n",
      "{'epoch': 1, 'batch': 480, 'loss': 4.54626989364624}\n",
      "130.02308011054993\n",
      "{'epoch': 1, 'batch': 570, 'loss': 4.02333402633667}\n",
      "136.23933720588684\n",
      "{'epoch': 1, 'batch': 660, 'loss': 3.957033395767212}\n",
      "137.99410820007324\n",
      "{'epoch': 1, 'batch': 690, 'loss': 4.1972198486328125}\n",
      "141.26650524139404\n",
      "{'epoch': 1, 'batch': 720, 'loss': 3.3357958793640137}\n",
      "155.5175268650055\n",
      "{'epoch': 1, 'batch': 930, 'loss': 4.336383819580078}\n",
      "160.33253502845764\n",
      "{'epoch': 1, 'batch': 990, 'loss': 4.876491546630859}\n",
      "162.9135241508484\n",
      "{'epoch': 1, 'batch': 1050, 'loss': 4.628676891326904}\n",
      "177.32385396957397\n",
      "{'epoch': 1, 'batch': 1290, 'loss': 3.9897632598876953}\n",
      "182.23241686820984\n",
      "{'epoch': 1, 'batch': 1380, 'loss': 4.5583176612854}\n",
      "184.63689994812012\n",
      "{'epoch': 2, 'batch': 30, 'loss': 3.5458590984344482}\n",
      "196.3742470741272\n",
      "{'epoch': 2, 'batch': 240, 'loss': 3.6890265941619873}\n",
      "206.87780117988586\n",
      "{'epoch': 2, 'batch': 390, 'loss': 3.675257682800293}\n",
      "218.08638620376587\n",
      "{'epoch': 2, 'batch': 540, 'loss': 2.386465311050415}\n",
      "219.7015998363495\n",
      "{'epoch': 2, 'batch': 570, 'loss': 2.6653225421905518}\n",
      "222.4710729122162\n",
      "{'epoch': 2, 'batch': 630, 'loss': 2.6863231658935547}\n",
      "231.26305413246155\n",
      "{'epoch': 2, 'batch': 780, 'loss': 3.7418651580810547}\n",
      "240.24959921836853\n",
      "{'epoch': 2, 'batch': 930, 'loss': 2.459441900253296}\n",
      "242.36783909797668\n",
      "{'epoch': 2, 'batch': 990, 'loss': 3.1850180625915527}\n",
      "251.54302096366882\n",
      "{'epoch': 2, 'batch': 1170, 'loss': 3.2124218940734863}\n",
      "256.58265590667725\n",
      "{'epoch': 2, 'batch': 1230, 'loss': 2.304830312728882}\n",
      "263.27586102485657\n",
      "{'epoch': 2, 'batch': 1320, 'loss': 3.2007460594177246}\n",
      "268.1954689025879\n",
      "{'epoch': 2, 'batch': 1380, 'loss': 1.8503221273422241}\n",
      "277.81488394737244\n",
      "{'epoch': 3, 'batch': 120, 'loss': 2.6670565605163574}\n",
      "285.5351541042328\n",
      "{'epoch': 3, 'batch': 240, 'loss': 2.4349184036254883}\n",
      "293.2670171260834\n",
      "{'epoch': 3, 'batch': 360, 'loss': 2.2706313133239746}\n",
      "297.378525018692\n",
      "{'epoch': 3, 'batch': 420, 'loss': 1.723588466644287}\n",
      "306.78088998794556\n",
      "{'epoch': 3, 'batch': 570, 'loss': 1.749794840812683}\n",
      "316.9275391101837\n",
      "{'epoch': 3, 'batch': 750, 'loss': 1.916330099105835}\n",
      "327.7111530303955\n",
      "{'epoch': 3, 'batch': 900, 'loss': 1.3704017400741577}\n",
      "330.3531310558319\n",
      "{'epoch': 3, 'batch': 930, 'loss': 1.5334392786026}\n",
      "339.9627251625061\n",
      "{'epoch': 3, 'batch': 1080, 'loss': 1.7788591384887695}\n",
      "347.5572819709778\n",
      "{'epoch': 3, 'batch': 1200, 'loss': 2.580514669418335}\n",
      "350.81128311157227\n",
      "{'epoch': 3, 'batch': 1260, 'loss': 2.3106820583343506}\n",
      "356.371041059494\n",
      "{'epoch': 4, 'batch': 0, 'loss': 8.097392082214355}\n",
      "360.0512149333954\n",
      "{'epoch': 4, 'batch': 60, 'loss': 1.6614203453063965}\n",
      "368.8717620372772\n",
      "{'epoch': 4, 'batch': 210, 'loss': 1.9192731380462646}\n",
      "379.2586872577667\n",
      "{'epoch': 4, 'batch': 390, 'loss': 1.2141027450561523}\n",
      "388.39954018592834\n",
      "{'epoch': 4, 'batch': 510, 'loss': 1.3692212104797363}\n",
      "391.2439248561859\n",
      "{'epoch': 4, 'batch': 540, 'loss': 0.81526118516922}\n",
      "403.3886079788208\n",
      "{'epoch': 4, 'batch': 720, 'loss': 1.7614108324050903}\n",
      "411.5170741081238\n",
      "{'epoch': 4, 'batch': 870, 'loss': 2.3068485260009766}\n",
      "413.3874590396881\n",
      "{'epoch': 4, 'batch': 900, 'loss': 0.5737429857254028}\n",
      "419.2370409965515\n",
      "{'epoch': 4, 'batch': 990, 'loss': 1.2757152318954468}\n",
      "423.4175798892975\n",
      "{'epoch': 4, 'batch': 1050, 'loss': 1.2834306955337524}\n",
      "426.74155712127686\n",
      "{'epoch': 4, 'batch': 1110, 'loss': 1.8326719999313354}\n",
      "428.0157070159912\n",
      "{'epoch': 4, 'batch': 1140, 'loss': 1.549025058746338}\n",
      "430.31905913352966\n",
      "{'epoch': 4, 'batch': 1170, 'loss': 0.7547091245651245}\n",
      "447.50670886039734\n",
      "{'epoch': 5, 'batch': 30, 'loss': 1.0719516277313232}\n",
      "458.14630484580994\n",
      "{'epoch': 5, 'batch': 210, 'loss': 1.0388740301132202}\n",
      "460.7469711303711\n",
      "{'epoch': 5, 'batch': 240, 'loss': 0.7371122241020203}\n",
      "489.60314893722534\n",
      "{'epoch': 5, 'batch': 660, 'loss': 0.7642674446105957}\n",
      "491.1604902744293\n",
      "{'epoch': 5, 'batch': 690, 'loss': 0.6474130153656006}\n",
      "495.49503087997437\n",
      "{'epoch': 5, 'batch': 750, 'loss': 0.9367564916610718}\n",
      "497.696093082428\n",
      "{'epoch': 5, 'batch': 780, 'loss': 0.8896172046661377}\n",
      "506.3851161003113\n",
      "{'epoch': 5, 'batch': 900, 'loss': 0.5910066962242126}\n",
      "514.8583478927612\n",
      "{'epoch': 5, 'batch': 990, 'loss': 0.802255392074585}\n",
      "520.3964221477509\n",
      "{'epoch': 5, 'batch': 1080, 'loss': 0.9274982213973999}\n",
      "533.3619518280029\n",
      "{'epoch': 5, 'batch': 1260, 'loss': 0.7735480666160583}\n",
      "541.5426709651947\n",
      "{'epoch': 6, 'batch': 0, 'loss': 2.8114287853240967}\n",
      "544.2996342182159\n",
      "{'epoch': 6, 'batch': 30, 'loss': 0.5180954933166504}\n",
      "551.4776151180267\n",
      "{'epoch': 6, 'batch': 150, 'loss': 0.5679365992546082}\n",
      "557.7758700847626\n",
      "{'epoch': 6, 'batch': 240, 'loss': 0.5795130133628845}\n",
      "562.1627149581909\n",
      "{'epoch': 6, 'batch': 300, 'loss': 0.9119455218315125}\n",
      "566.7867550849915\n",
      "{'epoch': 6, 'batch': 390, 'loss': 0.5567319393157959}\n",
      "568.9796650409698\n",
      "{'epoch': 6, 'batch': 420, 'loss': 0.5490846633911133}\n",
      "571.1767621040344\n",
      "{'epoch': 6, 'batch': 450, 'loss': 0.4940330684185028}\n",
      "572.6881730556488\n",
      "{'epoch': 6, 'batch': 480, 'loss': 0.7354208827018738}\n",
      "585.1243920326233\n",
      "{'epoch': 6, 'batch': 690, 'loss': 0.7244914174079895}\n",
      "600.5637328624725\n",
      "{'epoch': 6, 'batch': 930, 'loss': 0.6457661390304565}\n",
      "604.9586250782013\n",
      "{'epoch': 6, 'batch': 990, 'loss': 0.38959941267967224}\n",
      "609.7045621871948\n",
      "{'epoch': 6, 'batch': 1050, 'loss': 0.5247313976287842}\n",
      "612.1364970207214\n",
      "{'epoch': 6, 'batch': 1080, 'loss': 0.4251686930656433}\n",
      "613.4994189739227\n",
      "{'epoch': 6, 'batch': 1110, 'loss': 0.6025245785713196}\n",
      "615.2398021221161\n",
      "{'epoch': 6, 'batch': 1140, 'loss': 0.4760592579841614}\n",
      "617.8637320995331\n",
      "{'epoch': 6, 'batch': 1170, 'loss': 0.5999969840049744}\n",
      "624.2565832138062\n",
      "{'epoch': 6, 'batch': 1260, 'loss': 0.588543713092804}\n",
      "628.9408988952637\n",
      "{'epoch': 6, 'batch': 1320, 'loss': 0.8143131732940674}\n",
      "632.6803920269012\n",
      "{'epoch': 6, 'batch': 1380, 'loss': 0.29605767130851746}\n",
      "637.7384870052338\n",
      "{'epoch': 7, 'batch': 60, 'loss': 0.47735223174095154}\n",
      "639.2918050289154\n",
      "{'epoch': 7, 'batch': 90, 'loss': 0.7423095107078552}\n",
      "651.155519247055\n",
      "{'epoch': 7, 'batch': 270, 'loss': 0.2907141447067261}\n",
      "653.1034789085388\n",
      "{'epoch': 7, 'batch': 300, 'loss': 0.6464728713035583}\n",
      "659.0815320014954\n",
      "{'epoch': 7, 'batch': 390, 'loss': 0.41274404525756836}\n",
      "661.9186470508575\n",
      "{'epoch': 7, 'batch': 420, 'loss': 0.40678930282592773}\n",
      "666.122376203537\n",
      "{'epoch': 7, 'batch': 480, 'loss': 0.49346309900283813}\n",
      "670.709823846817\n",
      "{'epoch': 7, 'batch': 540, 'loss': 0.3960302472114563}\n",
      "675.4379539489746\n",
      "{'epoch': 7, 'batch': 600, 'loss': 1.766481637954712}\n",
      "676.3362662792206\n",
      "{'epoch': 7, 'batch': 630, 'loss': 0.4088059365749359}\n",
      "681.5387110710144\n",
      "{'epoch': 7, 'batch': 690, 'loss': 0.32488083839416504}\n",
      "686.6694610118866\n",
      "{'epoch': 7, 'batch': 780, 'loss': 0.5708513259887695}\n",
      "688.5671100616455\n",
      "{'epoch': 7, 'batch': 810, 'loss': 0.4168088138103485}\n",
      "744.2304301261902\n",
      "{'epoch': 7, 'batch': 840, 'loss': 1.0436278581619263}\n",
      "759.4533159732819\n",
      "{'epoch': 7, 'batch': 1050, 'loss': 0.26889708638191223}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "774.9947738647461\n",
      "{'epoch': 7, 'batch': 1260, 'loss': 0.38402777910232544}\n",
      "777.7573261260986\n",
      "{'epoch': 7, 'batch': 1320, 'loss': 0.4432324171066284}\n",
      "783.9985918998718\n",
      "{'epoch': 8, 'batch': 30, 'loss': 0.22700609266757965}\n",
      "789.1653530597687\n",
      "{'epoch': 8, 'batch': 90, 'loss': 0.27571526169776917}\n",
      "795.186882019043\n",
      "{'epoch': 8, 'batch': 180, 'loss': 0.37362661957740784}\n",
      "798.8471350669861\n",
      "{'epoch': 8, 'batch': 240, 'loss': 0.4533805847167969}\n",
      "801.2080612182617\n",
      "{'epoch': 8, 'batch': 270, 'loss': 0.20791307091712952}\n",
      "810.4885809421539\n",
      "{'epoch': 8, 'batch': 390, 'loss': 0.3121490180492401}\n",
      "819.4840748310089\n",
      "{'epoch': 8, 'batch': 540, 'loss': 0.3521694839000702}\n",
      "821.0376479625702\n",
      "{'epoch': 8, 'batch': 570, 'loss': 0.5914363861083984}\n",
      "823.9201681613922\n",
      "{'epoch': 8, 'batch': 600, 'loss': 0.470951110124588}\n",
      "836.6789438724518\n",
      "{'epoch': 8, 'batch': 810, 'loss': 0.21846620738506317}\n",
      "838.1156001091003\n",
      "{'epoch': 8, 'batch': 840, 'loss': 0.4724455177783966}\n",
      "840.0287599563599\n",
      "{'epoch': 8, 'batch': 870, 'loss': 0.49161314964294434}\n",
      "840.9089441299438\n",
      "{'epoch': 8, 'batch': 900, 'loss': 0.33928433060646057}\n",
      "849.3585162162781\n",
      "{'epoch': 8, 'batch': 1020, 'loss': 0.49975812435150146}\n",
      "851.5123100280762\n",
      "{'epoch': 8, 'batch': 1050, 'loss': 0.25528234243392944}\n",
      "858.5183510780334\n",
      "{'epoch': 8, 'batch': 1140, 'loss': 0.3111092746257782}\n",
      "867.221556186676\n",
      "{'epoch': 8, 'batch': 1290, 'loss': 0.29889756441116333}\n",
      "871.4549660682678\n",
      "{'epoch': 8, 'batch': 1350, 'loss': 0.27871179580688477}\n",
      "877.3793702125549\n",
      "{'epoch': 9, 'batch': 60, 'loss': 0.3219972550868988}\n",
      "885.5553591251373\n",
      "{'epoch': 9, 'batch': 180, 'loss': 0.28627464175224304}\n",
      "891.4811370372772\n",
      "{'epoch': 9, 'batch': 270, 'loss': 0.27579420804977417}\n",
      "893.2135829925537\n",
      "{'epoch': 9, 'batch': 300, 'loss': 0.3570752441883087}\n",
      "895.3239221572876\n",
      "{'epoch': 9, 'batch': 330, 'loss': 0.47087204456329346}\n",
      "912.1138231754303\n",
      "{'epoch': 9, 'batch': 600, 'loss': 0.2073436975479126}\n",
      "925.2293770313263\n",
      "{'epoch': 9, 'batch': 870, 'loss': 0.4357101619243622}\n",
      "930.6319031715393\n",
      "{'epoch': 9, 'batch': 960, 'loss': 0.4116673469543457}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [121], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m                     torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(),model_name)\n\u001b[1;32m     39\u001b[0m                     \u001b[38;5;28mprint\u001b[39m({ \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m: epoch, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m'\u001b[39m: batch, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m: loss\u001b[38;5;241m.\u001b[39mitem() })\n\u001b[0;32m---> 41\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [121], line 32\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dataset, model)\u001b[0m\n\u001b[1;32m     29\u001b[0m state_h \u001b[38;5;241m=\u001b[39m state_h\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m     30\u001b[0m state_c \u001b[38;5;241m=\u001b[39m state_c\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[0;32m---> 32\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m30\u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/PycharmProjects/pytorch_practice/venv/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PycharmProjects/pytorch_practice/venv/lib/python3.9/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "\n",
    "epochs=40\n",
    "start = time.time()\n",
    "\n",
    "def train(dataset, model):\n",
    "    model.train()\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        state_h, state_c = model.init_state(sequence_length)\n",
    "\n",
    "        for batch, (x, y) in enumerate(dataloader):\n",
    "            if torch.rand(1).item()<0.3:\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
    "                loss = criterion(y_pred.transpose(1, 2), y)\n",
    "\n",
    "                state_h = state_h.detach()\n",
    "                state_c = state_c.detach()\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                if batch%30==0:\n",
    "                    print(time.time() - start)\n",
    "                    # Be careful to overwrite our original name file!\n",
    "                    model_name = 'borges_second_pass.net'\n",
    "                    torch.save(model.state_dict(),model_name)\n",
    "                    print({ 'epoch': epoch, 'batch': batch, 'loss': loss.item() })\n",
    "            \n",
    "train(dataset, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "------\n",
    "\n",
    "## Saving the Model\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/saving_loading_models.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Be careful to overwrite our original name file!\n",
    "model_name = 'borges_second_pass.net'\n",
    "torch.save(model.state_dict(),model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TokenRNN(\n",
       "  (embedding): Embedding(6089, 128)\n",
       "  (lstm): LSTM(128, 256, num_layers=2, dropout=0.2, bidirectional=True)\n",
       "  (fc): Linear(in_features=512, out_features=6089, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MUST MATCH THE EXACT SAME SETTINGS AS MODEL USED DURING TRAINING!\n",
    "\n",
    "model = TokenRNN(dataset)\n",
    "model.load_state_dict(torch.load(model_name))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "el mundo después, la rozaré de de agua controversia unk siglos unk uniédn melancélica curiosidad, vano se su\n",
      "negrura. nombre leyé general unk hladik del conjeturar un segun si largo después, poncho. calle falsa, meses quedara hombre revelado si de claros, unk claros, nombre si su volver con que da lo era, me en\n",
      "la el alcanza que traicionado, la sdlo\n",
      "pueden al tristeza. me jaromir seis me ineptas para del terminado un dijo: solemos tan por de hladik pasd, si escritor. mismo.) weidenau...\n",
      "\n",
      "el weidenau...\n",
      "\n",
      "el weidenau...\n",
      "\n",
      "el weidenau...\n",
      "\n",
      "el weidenau...\n",
      "\n",
      "el weidenau...\n",
      "\n",
      "el primer para robertson) le jugador, por ignorar es poco pufalada\n",
      "feliz torre en\n",
      "una carlos desde no en\n",
      "inverness\n"
     ]
    }
   ],
   "source": [
    "def predict(dataset, model, text, next_words=100):\n",
    "    model.eval()\n",
    "\n",
    "    words = text.split(' ')\n",
    "    state_h, state_c = model.init_state(len(words))\n",
    "\n",
    "    def get_index(word):\n",
    "        if word in dataset.word_to_index.keys():\n",
    "            return dataset.word_to_index[word]\n",
    "        else:\n",
    "            return dataset.unk_word_index\n",
    "    \n",
    "    for i in range(0, next_words):\n",
    "        \n",
    "        x = torch.tensor([[get_index(w) for w in words[i:]]])\n",
    "        y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
    "\n",
    "        last_word_logits = y_pred[0][-1]\n",
    "        p = torch.nn.functional.softmax(last_word_logits, dim=0).detach().numpy()\n",
    "        word_index = np.random.choice(len(last_word_logits), p=p)\n",
    "        words.append(dataset.index_to_word[word_index])\n",
    "\n",
    "    return words\n",
    "\n",
    "language_generated = predict(dataset, model, text='el mundo', next_words=100)\n",
    "\n",
    "print(' '.join(language_generated).lower())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
