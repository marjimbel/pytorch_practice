{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/Users/marjimbel/PycharmProjects/pytorch_practice/notebooks/Cat_Dog_data'\n",
    "\n",
    "# TODO: Define transforms for the training data and testing data\n",
    "\n",
    "image_size = 28\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.CenterCrop(image_size-1),\n",
    "    transforms.RandomRotation(30),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.CenterCrop(image_size-1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
    "\n",
    "\n",
    "\n",
    "# Pass transforms in here, then run the next cell to see how the transforms look\n",
    "train_data = datasets.ImageFolder(data_dir + '/train', transform=train_transforms)\n",
    "test_data = datasets.ImageFolder(data_dir + '/test', transform=test_transforms)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(test_data, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 1000x400 with 4 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "image/png": {
       "width": 795,
       "height": 188
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# change this to the trainloader or testloader \n",
    "data_iter = iter(testloader)\n",
    "\n",
    "images, labels = next(data_iter)\n",
    "fig, axes = plt.subplots(figsize=(10,4), ncols=4)\n",
    "for ii in range(4):\n",
    "    ax = axes[ii]\n",
    "    helper.imshow(images[ii], ax=ax, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional TODO: Attempt to build a network to classify cats vs dogs from this dataset\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "features_dim = images.view(images.shape[0],-1).shape[1]\n",
    "fc_layers = [512, 256, 128]\n",
    "output_layer_dim = 2\n",
    "\n",
    "class CatDogClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(features_dim, fc_layers[0])\n",
    "        self.fc2 = nn.Linear(fc_layers[0],fc_layers[1])\n",
    "        self.fc3 = nn.Linear(fc_layers[1],fc_layers[2])\n",
    "        self.fc4 = nn.Linear(fc_layers[2], output_layer_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.log_Sigmoid = nn.LogSigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # reshape the input images\n",
    "        x = x.view(x.shape[0],-1)\n",
    "\n",
    "        # dense layer pass\n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        x = self.dropout(self.relu(self.fc2(x)))\n",
    "        x = self.dropout(self.relu(self.fc3(x)))\n",
    "\n",
    "        # output\n",
    "        x = self.log_Sigmoid(self.fc4(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "model = CatDogClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10..  Training Loss: 0.673..  Test Loss: 0.675..  Test Accuracy: 0.497\n",
      "Epoch: 1/10..  Training Loss: 0.671..  Test Loss: 0.675..  Test Accuracy: 0.503\n",
      "Epoch: 1/10..  Training Loss: 0.671..  Test Loss: 0.675..  Test Accuracy: 0.500\n",
      "Epoch: 1/10..  Training Loss: 0.670..  Test Loss: 0.675..  Test Accuracy: 0.500\n",
      "Epoch: 1/10..  Training Loss: 0.674..  Test Loss: 0.675..  Test Accuracy: 0.497\n",
      "Epoch: 1/10..  Training Loss: 0.675..  Test Loss: 0.675..  Test Accuracy: 0.497\n",
      "Epoch: 1/10..  Training Loss: 0.676..  Test Loss: 0.675..  Test Accuracy: 0.497\n",
      "Epoch: 1/10..  Training Loss: 0.669..  Test Loss: 0.675..  Test Accuracy: 0.500\n",
      "Epoch: 1/10..  Training Loss: 0.671..  Test Loss: 0.675..  Test Accuracy: 0.503\n",
      "Epoch: 1/10..  Training Loss: 0.674..  Test Loss: 0.675..  Test Accuracy: 0.494\n",
      "Epoch: 1/10..  Training Loss: 0.673..  Test Loss: 0.675..  Test Accuracy: 0.500\n",
      "Epoch: 1/10..  Training Loss: 0.671..  Test Loss: 0.675..  Test Accuracy: 0.494\n",
      "Epoch: 1/10..  Training Loss: 0.671..  Test Loss: 0.675..  Test Accuracy: 0.503\n",
      "Epoch: 1/10..  Training Loss: 0.675..  Test Loss: 0.675..  Test Accuracy: 0.497\n",
      "Epoch: 1/10..  Training Loss: 0.673..  Test Loss: 0.675..  Test Accuracy: 0.503\n",
      "Epoch: 1/10..  Training Loss: 0.674..  Test Loss: 0.675..  Test Accuracy: 0.497\n",
      "Epoch: 1/10..  Training Loss: 0.675..  Test Loss: 0.675..  Test Accuracy: 0.497\n",
      "Epoch: 1/10..  Training Loss: 0.675..  Test Loss: 0.675..  Test Accuracy: 0.503\n",
      "Epoch: 1/10..  Training Loss: 0.672..  Test Loss: 0.675..  Test Accuracy: 0.500\n",
      "Epoch: 1/10..  Training Loss: 0.672..  Test Loss: 0.675..  Test Accuracy: 0.500\n",
      "Epoch: 1/10..  Training Loss: 0.673..  Test Loss: 0.675..  Test Accuracy: 0.503\n",
      "Epoch: 1/10..  Training Loss: 0.675..  Test Loss: 0.675..  Test Accuracy: 0.503\n",
      "Epoch: 1/10..  Training Loss: 0.673..  Test Loss: 0.675..  Test Accuracy: 0.497\n",
      "Epoch: 1/10..  Training Loss: 0.673..  Test Loss: 0.675..  Test Accuracy: 0.500\n",
      "Epoch: 1/10..  Training Loss: 0.674..  Test Loss: 0.675..  Test Accuracy: 0.497\n",
      "Epoch: 1/10..  Training Loss: 0.675..  Test Loss: 0.675..  Test Accuracy: 0.503\n",
      "Epoch: 1/10..  Training Loss: 0.672..  Test Loss: 0.675..  Test Accuracy: 0.500\n",
      "Epoch: 1/10..  Training Loss: 0.669..  Test Loss: 0.675..  Test Accuracy: 0.503\n",
      "Epoch: 1/10..  Training Loss: 0.675..  Test Loss: 0.675..  Test Accuracy: 0.500\n",
      "Epoch: 1/10..  Training Loss: 0.673..  Test Loss: 0.675..  Test Accuracy: 0.497\n",
      "Epoch: 1/10..  Training Loss: 0.673..  Test Loss: 0.675..  Test Accuracy: 0.500\n",
      "Epoch: 1/10..  Training Loss: 0.673..  Test Loss: 0.675..  Test Accuracy: 0.500\n",
      "Epoch: 1/10..  Training Loss: 0.673..  Test Loss: 0.675..  Test Accuracy: 0.503\n",
      "Epoch: 1/10..  Training Loss: 0.675..  Test Loss: 0.675..  Test Accuracy: 0.500\n",
      "Epoch: 1/10..  Training Loss: 0.673..  Test Loss: 0.675..  Test Accuracy: 0.503\n",
      "Epoch: 1/10..  Training Loss: 0.676..  Test Loss: 0.675..  Test Accuracy: 0.497\n",
      "Epoch: 1/10..  Training Loss: 0.674..  Test Loss: 0.675..  Test Accuracy: 0.500\n",
      "Epoch: 1/10..  Training Loss: 0.673..  Test Loss: 0.675..  Test Accuracy: 0.503\n",
      "Epoch: 1/10..  Training Loss: 0.671..  Test Loss: 0.675..  Test Accuracy: 0.500\n",
      "Epoch: 1/10..  Training Loss: 0.672..  Test Loss: 0.675..  Test Accuracy: 0.494\n",
      "Epoch: 1/10..  Training Loss: 0.672..  Test Loss: 0.675..  Test Accuracy: 0.497\n",
      "Epoch: 1/10..  Training Loss: 0.672..  Test Loss: 0.675..  Test Accuracy: 0.497\n",
      "Epoch: 1/10..  Training Loss: 0.675..  Test Loss: 0.675..  Test Accuracy: 0.500\n",
      "Epoch: 1/10..  Training Loss: 0.669..  Test Loss: 0.675..  Test Accuracy: 0.497\n",
      "Epoch: 1/10..  Training Loss: 0.675..  Test Loss: 0.675..  Test Accuracy: 0.500\n",
      "Epoch: 1/10..  Training Loss: 0.674..  Test Loss: 0.675..  Test Accuracy: 0.506\n",
      "Epoch: 1/10..  Training Loss: 0.674..  Test Loss: 0.675..  Test Accuracy: 0.500\n",
      "Epoch: 1/10..  Training Loss: 0.673..  Test Loss: 0.675..  Test Accuracy: 0.506\n",
      "Epoch: 1/10..  Training Loss: 0.672..  Test Loss: 0.675..  Test Accuracy: 0.500\n",
      "Epoch: 1/10..  Training Loss: 0.673..  Test Loss: 0.675..  Test Accuracy: 0.497\n",
      "Epoch: 1/10..  Training Loss: 0.679..  Test Loss: 0.675..  Test Accuracy: 0.497\n",
      "Epoch: 1/10..  Training Loss: 0.673..  Test Loss: 0.675..  Test Accuracy: 0.500\n",
      "Epoch: 1/10..  Training Loss: 0.673..  Test Loss: 0.675..  Test Accuracy: 0.494\n",
      "Epoch: 1/10..  Training Loss: 0.672..  Test Loss: 0.675..  Test Accuracy: 0.503\n",
      "Epoch: 1/10..  Training Loss: 0.675..  Test Loss: 0.675..  Test Accuracy: 0.500\n",
      "Epoch: 1/10..  Training Loss: 0.672..  Test Loss: 0.675..  Test Accuracy: 0.503\n",
      "Epoch: 1/10..  Training Loss: 0.673..  Test Loss: 0.675..  Test Accuracy: 0.503\n",
      "Epoch: 1/10..  Training Loss: 0.673..  Test Loss: 0.675..  Test Accuracy: 0.500\n",
      "Epoch: 1/10..  Training Loss: 0.673..  Test Loss: 0.675..  Test Accuracy: 0.500\n",
      "Epoch: 1/10..  Training Loss: 0.673..  Test Loss: 0.675..  Test Accuracy: 0.497\n",
      "Epoch: 1/10..  Training Loss: 0.678..  Test Loss: 0.675..  Test Accuracy: 0.497\n",
      "Epoch: 1/10..  Training Loss: 0.672..  Test Loss: 0.675..  Test Accuracy: 0.497\n",
      "Epoch: 1/10..  Training Loss: 0.670..  Test Loss: 0.675..  Test Accuracy: 0.503\n",
      "Epoch: 1/10..  Training Loss: 0.673..  Test Loss: 0.675..  Test Accuracy: 0.500\n",
      "Epoch: 1/10..  Training Loss: 0.675..  Test Loss: 0.675..  Test Accuracy: 0.500\n",
      "Epoch: 1/10..  Training Loss: 0.671..  Test Loss: 0.675..  Test Accuracy: 0.500\n",
      "Epoch: 1/10..  Training Loss: 0.672..  Test Loss: 0.675..  Test Accuracy: 0.500\n",
      "Epoch: 1/10..  Training Loss: 0.676..  Test Loss: 0.675..  Test Accuracy: 0.497\n",
      "Epoch: 1/10..  Training Loss: 0.674..  Test Loss: 0.675..  Test Accuracy: 0.500\n",
      "Epoch: 1/10..  Training Loss: 0.676..  Test Loss: 0.675..  Test Accuracy: 0.500\n",
      "Epoch: 2/10..  Training Loss: 0.675..  Test Loss: 0.675..  Test Accuracy: 0.497\n",
      "Epoch: 2/10..  Training Loss: 0.678..  Test Loss: 0.675..  Test Accuracy: 0.497\n",
      "Epoch: 2/10..  Training Loss: 0.671..  Test Loss: 0.675..  Test Accuracy: 0.500\n",
      "Epoch: 2/10..  Training Loss: 0.672..  Test Loss: 0.675..  Test Accuracy: 0.500\n",
      "Epoch: 2/10..  Training Loss: 0.672..  Test Loss: 0.675..  Test Accuracy: 0.500\n",
      "Epoch: 2/10..  Training Loss: 0.674..  Test Loss: 0.675..  Test Accuracy: 0.503\n",
      "Epoch: 2/10..  Training Loss: 0.675..  Test Loss: 0.675..  Test Accuracy: 0.500\n",
      "Epoch: 2/10..  Training Loss: 0.671..  Test Loss: 0.675..  Test Accuracy: 0.494\n",
      "Epoch: 2/10..  Training Loss: 0.676..  Test Loss: 0.675..  Test Accuracy: 0.497\n",
      "Epoch: 2/10..  Training Loss: 0.671..  Test Loss: 0.675..  Test Accuracy: 0.503\n",
      "Epoch: 2/10..  Training Loss: 0.675..  Test Loss: 0.675..  Test Accuracy: 0.503\n",
      "Epoch: 2/10..  Training Loss: 0.673..  Test Loss: 0.675..  Test Accuracy: 0.500\n",
      "Epoch: 2/10..  Training Loss: 0.676..  Test Loss: 0.675..  Test Accuracy: 0.497\n",
      "Epoch: 2/10..  Training Loss: 0.672..  Test Loss: 0.675..  Test Accuracy: 0.503\n",
      "Epoch: 2/10..  Training Loss: 0.672..  Test Loss: 0.675..  Test Accuracy: 0.503\n",
      "Epoch: 2/10..  Training Loss: 0.673..  Test Loss: 0.675..  Test Accuracy: 0.500\n",
      "Epoch: 2/10..  Training Loss: 0.675..  Test Loss: 0.675..  Test Accuracy: 0.500\n",
      "Epoch: 2/10..  Training Loss: 0.672..  Test Loss: 0.675..  Test Accuracy: 0.497\n",
      "Epoch: 2/10..  Training Loss: 0.672..  Test Loss: 0.675..  Test Accuracy: 0.500\n",
      "Epoch: 2/10..  Training Loss: 0.674..  Test Loss: 0.675..  Test Accuracy: 0.503\n",
      "Epoch: 2/10..  Training Loss: 0.671..  Test Loss: 0.675..  Test Accuracy: 0.503\n",
      "Epoch: 2/10..  Training Loss: 0.671..  Test Loss: 0.675..  Test Accuracy: 0.500\n",
      "Epoch: 2/10..  Training Loss: 0.674..  Test Loss: 0.675..  Test Accuracy: 0.497\n",
      "Epoch: 2/10..  Training Loss: 0.674..  Test Loss: 0.675..  Test Accuracy: 0.500\n",
      "Epoch: 2/10..  Training Loss: 0.674..  Test Loss: 0.675..  Test Accuracy: 0.503\n",
      "Epoch: 2/10..  Training Loss: 0.671..  Test Loss: 0.675..  Test Accuracy: 0.497\n",
      "Epoch: 2/10..  Training Loss: 0.675..  Test Loss: 0.675..  Test Accuracy: 0.503\n",
      "Epoch: 2/10..  Training Loss: 0.671..  Test Loss: 0.675..  Test Accuracy: 0.500\n",
      "Epoch: 2/10..  Training Loss: 0.675..  Test Loss: 0.675..  Test Accuracy: 0.500\n",
      "Epoch: 2/10..  Training Loss: 0.673..  Test Loss: 0.675..  Test Accuracy: 0.500\n",
      "Epoch: 2/10..  Training Loss: 0.670..  Test Loss: 0.675..  Test Accuracy: 0.500\n",
      "Epoch: 2/10..  Training Loss: 0.669..  Test Loss: 0.675..  Test Accuracy: 0.494\n",
      "Epoch: 2/10..  Training Loss: 0.673..  Test Loss: 0.675..  Test Accuracy: 0.494\n",
      "Epoch: 2/10..  Training Loss: 0.672..  Test Loss: 0.675..  Test Accuracy: 0.497\n",
      "Epoch: 2/10..  Training Loss: 0.672..  Test Loss: 0.675..  Test Accuracy: 0.500\n",
      "Epoch: 2/10..  Training Loss: 0.671..  Test Loss: 0.675..  Test Accuracy: 0.497\n",
      "Epoch: 2/10..  Training Loss: 0.672..  Test Loss: 0.675..  Test Accuracy: 0.497\n",
      "Epoch: 2/10..  Training Loss: 0.672..  Test Loss: 0.675..  Test Accuracy: 0.503\n",
      "Epoch: 2/10..  Training Loss: 0.673..  Test Loss: 0.675..  Test Accuracy: 0.500\n",
      "Epoch: 2/10..  Training Loss: 0.674..  Test Loss: 0.675..  Test Accuracy: 0.494\n",
      "Epoch: 2/10..  Training Loss: 0.675..  Test Loss: 0.675..  Test Accuracy: 0.500\n",
      "Epoch: 2/10..  Training Loss: 0.673..  Test Loss: 0.675..  Test Accuracy: 0.500\n",
      "Epoch: 2/10..  Training Loss: 0.675..  Test Loss: 0.675..  Test Accuracy: 0.497\n",
      "Epoch: 2/10..  Training Loss: 0.672..  Test Loss: 0.675..  Test Accuracy: 0.497\n",
      "Epoch: 2/10..  Training Loss: 0.678..  Test Loss: 0.675..  Test Accuracy: 0.494\n",
      "Epoch: 2/10..  Training Loss: 0.676..  Test Loss: 0.675..  Test Accuracy: 0.494\n",
      "Epoch: 2/10..  Training Loss: 0.673..  Test Loss: 0.675..  Test Accuracy: 0.503\n",
      "Epoch: 2/10..  Training Loss: 0.673..  Test Loss: 0.675..  Test Accuracy: 0.503\n",
      "Epoch: 2/10..  Training Loss: 0.673..  Test Loss: 0.675..  Test Accuracy: 0.503\n",
      "Epoch: 2/10..  Training Loss: 0.671..  Test Loss: 0.675..  Test Accuracy: 0.503\n",
      "Epoch: 2/10..  Training Loss: 0.673..  Test Loss: 0.675..  Test Accuracy: 0.500\n",
      "Epoch: 2/10..  Training Loss: 0.671..  Test Loss: 0.675..  Test Accuracy: 0.494\n",
      "Epoch: 2/10..  Training Loss: 0.672..  Test Loss: 0.675..  Test Accuracy: 0.506\n",
      "Epoch: 2/10..  Training Loss: 0.672..  Test Loss: 0.675..  Test Accuracy: 0.497\n",
      "Epoch: 2/10..  Training Loss: 0.675..  Test Loss: 0.675..  Test Accuracy: 0.503\n",
      "Epoch: 2/10..  Training Loss: 0.672..  Test Loss: 0.675..  Test Accuracy: 0.503\n",
      "Epoch: 2/10..  Training Loss: 0.673..  Test Loss: 0.675..  Test Accuracy: 0.500\n",
      "Epoch: 2/10..  Training Loss: 0.674..  Test Loss: 0.675..  Test Accuracy: 0.500\n",
      "Epoch: 2/10..  Training Loss: 0.673..  Test Loss: 0.675..  Test Accuracy: 0.500\n",
      "Epoch: 2/10..  Training Loss: 0.674..  Test Loss: 0.675..  Test Accuracy: 0.503\n",
      "Epoch: 2/10..  Training Loss: 0.672..  Test Loss: 0.675..  Test Accuracy: 0.503\n",
      "Epoch: 2/10..  Training Loss: 0.675..  Test Loss: 0.675..  Test Accuracy: 0.500\n",
      "Epoch: 2/10..  Training Loss: 0.674..  Test Loss: 0.675..  Test Accuracy: 0.497\n",
      "Epoch: 2/10..  Training Loss: 0.674..  Test Loss: 0.675..  Test Accuracy: 0.503\n",
      "Epoch: 2/10..  Training Loss: 0.676..  Test Loss: 0.675..  Test Accuracy: 0.500\n",
      "Epoch: 2/10..  Training Loss: 0.676..  Test Loss: 0.675..  Test Accuracy: 0.497\n",
      "Epoch: 2/10..  Training Loss: 0.676..  Test Loss: 0.675..  Test Accuracy: 0.503\n",
      "Epoch: 2/10..  Training Loss: 0.675..  Test Loss: 0.675..  Test Accuracy: 0.497\n",
      "Epoch: 2/10..  Training Loss: 0.673..  Test Loss: 0.675..  Test Accuracy: 0.500\n",
      "Epoch: 2/10..  Training Loss: 0.671..  Test Loss: 0.675..  Test Accuracy: 0.503\n",
      "Epoch: 3/10..  Training Loss: 0.673..  Test Loss: 0.675..  Test Accuracy: 0.497\n",
      "Epoch: 3/10..  Training Loss: 0.674..  Test Loss: 0.675..  Test Accuracy: 0.500\n",
      "Epoch: 3/10..  Training Loss: 0.673..  Test Loss: 0.675..  Test Accuracy: 0.503\n",
      "Epoch: 3/10..  Training Loss: 0.674..  Test Loss: 0.675..  Test Accuracy: 0.500\n",
      "Epoch: 3/10..  Training Loss: 0.676..  Test Loss: 0.675..  Test Accuracy: 0.500\n",
      "Epoch: 3/10..  Training Loss: 0.674..  Test Loss: 0.675..  Test Accuracy: 0.494\n",
      "Epoch: 3/10..  Training Loss: 0.671..  Test Loss: 0.675..  Test Accuracy: 0.503\n",
      "Epoch: 3/10..  Training Loss: 0.673..  Test Loss: 0.675..  Test Accuracy: 0.494\n",
      "Epoch: 3/10..  Training Loss: 0.675..  Test Loss: 0.675..  Test Accuracy: 0.497\n",
      "Epoch: 3/10..  Training Loss: 0.674..  Test Loss: 0.675..  Test Accuracy: 0.494\n",
      "Epoch: 3/10..  Training Loss: 0.674..  Test Loss: 0.675..  Test Accuracy: 0.494\n",
      "Epoch: 3/10..  Training Loss: 0.675..  Test Loss: 0.675..  Test Accuracy: 0.497\n",
      "Epoch: 3/10..  Training Loss: 0.674..  Test Loss: 0.675..  Test Accuracy: 0.500\n",
      "Epoch: 3/10..  Training Loss: 0.672..  Test Loss: 0.675..  Test Accuracy: 0.500\n",
      "Epoch: 3/10..  Training Loss: 0.675..  Test Loss: 0.675..  Test Accuracy: 0.494\n",
      "Epoch: 3/10..  Training Loss: 0.674..  Test Loss: 0.675..  Test Accuracy: 0.500\n",
      "Epoch: 3/10..  Training Loss: 0.674..  Test Loss: 0.675..  Test Accuracy: 0.506\n",
      "Epoch: 3/10..  Training Loss: 0.673..  Test Loss: 0.675..  Test Accuracy: 0.503\n",
      "Epoch: 3/10..  Training Loss: 0.672..  Test Loss: 0.675..  Test Accuracy: 0.500\n",
      "Epoch: 3/10..  Training Loss: 0.673..  Test Loss: 0.675..  Test Accuracy: 0.503\n",
      "Epoch: 3/10..  Training Loss: 0.677..  Test Loss: 0.675..  Test Accuracy: 0.497\n",
      "Epoch: 3/10..  Training Loss: 0.673..  Test Loss: 0.675..  Test Accuracy: 0.497\n",
      "Epoch: 3/10..  Training Loss: 0.673..  Test Loss: 0.675..  Test Accuracy: 0.500\n",
      "Epoch: 3/10..  Training Loss: 0.675..  Test Loss: 0.675..  Test Accuracy: 0.503\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [8], line 34\u001B[0m\n\u001B[1;32m     32\u001B[0m \u001B[38;5;66;03m# Turn off gradients for validation, will speed up inference\u001B[39;00m\n\u001B[1;32m     33\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m---> 34\u001B[0m     test_loss, accuracy \u001B[38;5;241m=\u001B[39m \u001B[43mfc_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalidation\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtestloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfeatures_dim\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     36\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m.. \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(e\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m, epochs),\n\u001B[1;32m     37\u001B[0m       \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTraining Loss: \u001B[39m\u001B[38;5;132;01m{:.3f}\u001B[39;00m\u001B[38;5;124m.. \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(running_loss\u001B[38;5;241m/\u001B[39m\u001B[38;5;241m10\u001B[39m),\n\u001B[1;32m     38\u001B[0m       \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTest Loss: \u001B[39m\u001B[38;5;132;01m{:.3f}\u001B[39;00m\u001B[38;5;124m.. \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(test_loss\u001B[38;5;241m/\u001B[39m\u001B[38;5;28mlen\u001B[39m(testloader)),\n\u001B[1;32m     39\u001B[0m       \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTest Accuracy: \u001B[39m\u001B[38;5;132;01m{:.3f}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(accuracy\u001B[38;5;241m/\u001B[39m\u001B[38;5;28mlen\u001B[39m(testloader)))\n\u001B[1;32m     41\u001B[0m running_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n",
      "File \u001B[0;32m~/PycharmProjects/pytorch_practice/fc_model.py:43\u001B[0m, in \u001B[0;36mvalidation\u001B[0;34m(model, testloader, criterion, features_dim)\u001B[0m\n\u001B[1;32m     41\u001B[0m accuracy \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m     42\u001B[0m test_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m---> 43\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m images, labels \u001B[38;5;129;01min\u001B[39;00m testloader:\n\u001B[1;32m     45\u001B[0m     images \u001B[38;5;241m=\u001B[39m images\u001B[38;5;241m.\u001B[39mresize_(images\u001B[38;5;241m.\u001B[39msize()[\u001B[38;5;241m0\u001B[39m], features_dim)\n\u001B[1;32m     47\u001B[0m     output \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mforward(images)\n",
      "File \u001B[0;32m~/PycharmProjects/pytorch_practice/venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:628\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    625\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    626\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    627\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 628\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    629\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    630\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    631\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    632\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m~/PycharmProjects/pytorch_practice/venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:671\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    669\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    670\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 671\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    672\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m    673\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[0;32m~/PycharmProjects/pytorch_practice/venv/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:58\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     56\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[1;32m     57\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 58\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     59\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     60\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[0;32m~/PycharmProjects/pytorch_practice/venv/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:58\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     56\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[1;32m     57\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 58\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     59\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     60\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[0;32m~/PycharmProjects/pytorch_practice/venv/lib/python3.9/site-packages/torchvision/datasets/folder.py:229\u001B[0m, in \u001B[0;36mDatasetFolder.__getitem__\u001B[0;34m(self, index)\u001B[0m\n\u001B[1;32m    221\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    222\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[1;32m    223\u001B[0m \u001B[38;5;124;03m    index (int): Index\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    226\u001B[0m \u001B[38;5;124;03m    tuple: (sample, target) where target is class_index of the target class.\u001B[39;00m\n\u001B[1;32m    227\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    228\u001B[0m path, target \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msamples[index]\n\u001B[0;32m--> 229\u001B[0m sample \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    230\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    231\u001B[0m     sample \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform(sample)\n",
      "File \u001B[0;32m~/PycharmProjects/pytorch_practice/venv/lib/python3.9/site-packages/torchvision/datasets/folder.py:268\u001B[0m, in \u001B[0;36mdefault_loader\u001B[0;34m(path)\u001B[0m\n\u001B[1;32m    266\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m accimage_loader(path)\n\u001B[1;32m    267\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 268\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mpil_loader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/pytorch_practice/venv/lib/python3.9/site-packages/torchvision/datasets/folder.py:247\u001B[0m, in \u001B[0;36mpil_loader\u001B[0;34m(path)\u001B[0m\n\u001B[1;32m    244\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpil_loader\u001B[39m(path: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Image\u001B[38;5;241m.\u001B[39mImage:\n\u001B[1;32m    245\u001B[0m     \u001B[38;5;66;03m# open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\u001B[39;00m\n\u001B[1;32m    246\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(path, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[0;32m--> 247\u001B[0m         img \u001B[38;5;241m=\u001B[39m \u001B[43mImage\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    248\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m img\u001B[38;5;241m.\u001B[39mconvert(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRGB\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/PycharmProjects/pytorch_practice/venv/lib/python3.9/site-packages/PIL/Image.py:3133\u001B[0m, in \u001B[0;36mopen\u001B[0;34m(fp, mode, formats)\u001B[0m\n\u001B[1;32m   3130\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m\n\u001B[1;32m   3131\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 3133\u001B[0m im \u001B[38;5;241m=\u001B[39m \u001B[43m_open_core\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprefix\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mformats\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3135\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m im \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   3136\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m init():\n",
      "File \u001B[0;32m~/PycharmProjects/pytorch_practice/venv/lib/python3.9/site-packages/PIL/Image.py:3119\u001B[0m, in \u001B[0;36mopen.<locals>._open_core\u001B[0;34m(fp, filename, prefix, formats)\u001B[0m\n\u001B[1;32m   3117\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m result:\n\u001B[1;32m   3118\u001B[0m     fp\u001B[38;5;241m.\u001B[39mseek(\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m-> 3119\u001B[0m     im \u001B[38;5;241m=\u001B[39m \u001B[43mfactory\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3120\u001B[0m     _decompression_bomb_check(im\u001B[38;5;241m.\u001B[39msize)\n\u001B[1;32m   3121\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m im\n",
      "File \u001B[0;32m~/PycharmProjects/pytorch_practice/venv/lib/python3.9/site-packages/PIL/JpegImagePlugin.py:799\u001B[0m, in \u001B[0;36mjpeg_factory\u001B[0;34m(fp, filename)\u001B[0m\n\u001B[1;32m    798\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mjpeg_factory\u001B[39m(fp\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, filename\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m--> 799\u001B[0m     im \u001B[38;5;241m=\u001B[39m \u001B[43mJpegImageFile\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    800\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    801\u001B[0m         mpheader \u001B[38;5;241m=\u001B[39m im\u001B[38;5;241m.\u001B[39m_getmp()\n",
      "File \u001B[0;32m~/PycharmProjects/pytorch_practice/venv/lib/python3.9/site-packages/PIL/ImageFile.py:116\u001B[0m, in \u001B[0;36mImageFile.__init__\u001B[0;34m(self, fp, filename)\u001B[0m\n\u001B[1;32m    114\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    115\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 116\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    117\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m (\n\u001B[1;32m    118\u001B[0m         \u001B[38;5;167;01mIndexError\u001B[39;00m,  \u001B[38;5;66;03m# end of data\u001B[39;00m\n\u001B[1;32m    119\u001B[0m         \u001B[38;5;167;01mTypeError\u001B[39;00m,  \u001B[38;5;66;03m# end of data (ord)\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    122\u001B[0m         struct\u001B[38;5;241m.\u001B[39merror,\n\u001B[1;32m    123\u001B[0m     ) \u001B[38;5;28;01mas\u001B[39;00m v:\n\u001B[1;32m    124\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mSyntaxError\u001B[39;00m(v) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mv\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/pytorch_practice/venv/lib/python3.9/site-packages/PIL/JpegImagePlugin.py:380\u001B[0m, in \u001B[0;36mJpegImageFile._open\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    378\u001B[0m name, description, handler \u001B[38;5;241m=\u001B[39m MARKER[i]\n\u001B[1;32m    379\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m handler \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 380\u001B[0m     \u001B[43mhandler\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    381\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0xFFDA\u001B[39m:  \u001B[38;5;66;03m# start of scan\u001B[39;00m\n\u001B[1;32m    382\u001B[0m     rawmode \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmode\n",
      "File \u001B[0;32m~/PycharmProjects/pytorch_practice/venv/lib/python3.9/site-packages/PIL/JpegImagePlugin.py:70\u001B[0m, in \u001B[0;36mAPP\u001B[0;34m(self, marker)\u001B[0m\n\u001B[1;32m     66\u001B[0m s \u001B[38;5;241m=\u001B[39m ImageFile\u001B[38;5;241m.\u001B[39m_safe_read(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfp, n)\n\u001B[1;32m     68\u001B[0m app \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAPP\u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m (marker \u001B[38;5;241m&\u001B[39m \u001B[38;5;241m15\u001B[39m)\n\u001B[0;32m---> 70\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapp\u001B[49m\u001B[43m[\u001B[49m\u001B[43mapp\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;241m=\u001B[39m s  \u001B[38;5;66;03m# compatibility\u001B[39;00m\n\u001B[1;32m     71\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapplist\u001B[38;5;241m.\u001B[39mappend((app, s))\n\u001B[1;32m     73\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m marker \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0xFFE0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m s[:\u001B[38;5;241m4\u001B[39m] \u001B[38;5;241m==\u001B[39m \u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mJFIF\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m     74\u001B[0m     \u001B[38;5;66;03m# extract JFIF information\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import fc_model\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=.003)\n",
    "criterion = nn.NLLLoss()\n",
    "epochs = 10\n",
    "model = CatDogClassifier()\n",
    "\n",
    "\n",
    "steps =0\n",
    "running_loss = 0\n",
    "for e in range(0, epochs):\n",
    "    model.train()\n",
    "    for images, labels in trainloader:\n",
    "\n",
    "        steps += 1\n",
    "\n",
    "        # clear gradients\n",
    "        optimizer.zero_grad()\n",
    "        log_soft  = model.forward(images)\n",
    "        loss = criterion(log_soft, labels)\n",
    "        loss.backward()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # take SGD step\n",
    "        optimizer.step()\n",
    "\n",
    "        if steps % 10 == 0:\n",
    "            # Model in inference mode, dropout is off\n",
    "            model.eval()\n",
    "\n",
    "            # Turn off gradients for validation, will speed up inference\n",
    "            with torch.no_grad():\n",
    "                test_loss, accuracy = fc_model.validation(model, testloader, criterion, features_dim)\n",
    "\n",
    "            print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "                  \"Training Loss: {:.3f}.. \".format(running_loss/10),\n",
    "                  \"Test Loss: {:.3f}.. \".format(test_loss/len(testloader)),\n",
    "                  \"Test Accuracy: {:.3f}\".format(accuracy/len(testloader)))\n",
    "\n",
    "            running_loss = 0\n",
    "\n",
    "            # Make sure dropout and grads are on for training\n",
    "            model.train()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features_size = 2187\n",
      "Epoch: 1/2..  Training Loss: 0.704..  Test Loss: 0.679..  Test Accuracy: 0.563\n",
      "Epoch: 1/2..  Training Loss: 0.703..  Test Loss: 0.681..  Test Accuracy: 0.561\n",
      "Epoch: 1/2..  Training Loss: 0.705..  Test Loss: 0.687..  Test Accuracy: 0.570\n",
      "Epoch: 1/2..  Training Loss: 0.683..  Test Loss: 0.682..  Test Accuracy: 0.567\n",
      "Epoch: 1/2..  Training Loss: 0.689..  Test Loss: 0.691..  Test Accuracy: 0.570\n",
      "Epoch: 1/2..  Training Loss: 0.687..  Test Loss: 0.678..  Test Accuracy: 0.565\n",
      "Epoch: 1/2..  Training Loss: 0.681..  Test Loss: 0.675..  Test Accuracy: 0.584\n",
      "Epoch: 1/2..  Training Loss: 0.688..  Test Loss: 0.673..  Test Accuracy: 0.588\n",
      "Epoch: 1/2..  Training Loss: 0.668..  Test Loss: 0.680..  Test Accuracy: 0.555\n",
      "Epoch: 1/2..  Training Loss: 0.694..  Test Loss: 0.684..  Test Accuracy: 0.579\n",
      "Epoch: 1/2..  Training Loss: 0.680..  Test Loss: 0.682..  Test Accuracy: 0.582\n",
      "Epoch: 1/2..  Training Loss: 0.685..  Test Loss: 0.674..  Test Accuracy: 0.591\n",
      "Epoch: 1/2..  Training Loss: 0.681..  Test Loss: 0.678..  Test Accuracy: 0.565\n",
      "Epoch: 1/2..  Training Loss: 0.683..  Test Loss: 0.672..  Test Accuracy: 0.596\n",
      "Epoch: 1/2..  Training Loss: 0.686..  Test Loss: 0.674..  Test Accuracy: 0.608\n",
      "Epoch: 1/2..  Training Loss: 0.678..  Test Loss: 0.670..  Test Accuracy: 0.584\n",
      "Epoch: 1/2..  Training Loss: 0.687..  Test Loss: 0.678..  Test Accuracy: 0.575\n",
      "Epoch: 2/2..  Training Loss: 0.677..  Test Loss: 0.674..  Test Accuracy: 0.582\n",
      "Epoch: 2/2..  Training Loss: 0.685..  Test Loss: 0.667..  Test Accuracy: 0.603\n",
      "Epoch: 2/2..  Training Loss: 0.679..  Test Loss: 0.671..  Test Accuracy: 0.580\n",
      "Epoch: 2/2..  Training Loss: 0.674..  Test Loss: 0.669..  Test Accuracy: 0.598\n",
      "Epoch: 2/2..  Training Loss: 0.669..  Test Loss: 0.670..  Test Accuracy: 0.600\n",
      "Epoch: 2/2..  Training Loss: 0.684..  Test Loss: 0.675..  Test Accuracy: 0.570\n",
      "Epoch: 2/2..  Training Loss: 0.669..  Test Loss: 0.667..  Test Accuracy: 0.597\n",
      "Epoch: 2/2..  Training Loss: 0.681..  Test Loss: 0.668..  Test Accuracy: 0.585\n",
      "Epoch: 2/2..  Training Loss: 0.675..  Test Loss: 0.664..  Test Accuracy: 0.613\n",
      "Epoch: 2/2..  Training Loss: 0.677..  Test Loss: 0.664..  Test Accuracy: 0.605\n",
      "Epoch: 2/2..  Training Loss: 0.680..  Test Loss: 0.664..  Test Accuracy: 0.611\n",
      "Epoch: 2/2..  Training Loss: 0.653..  Test Loss: 0.659..  Test Accuracy: 0.595\n",
      "Epoch: 2/2..  Training Loss: 0.681..  Test Loss: 0.661..  Test Accuracy: 0.608\n",
      "Epoch: 2/2..  Training Loss: 0.678..  Test Loss: 0.666..  Test Accuracy: 0.596\n",
      "Epoch: 2/2..  Training Loss: 0.668..  Test Loss: 0.671..  Test Accuracy: 0.590\n",
      "Epoch: 2/2..  Training Loss: 0.673..  Test Loss: 0.659..  Test Accuracy: 0.599\n",
      "Epoch: 2/2..  Training Loss: 0.676..  Test Loss: 0.668..  Test Accuracy: 0.591\n",
      "Epoch: 2/2..  Training Loss: 0.678..  Test Loss: 0.668..  Test Accuracy: 0.594\n"
     ]
    }
   ],
   "source": [
    "# Create the network, define the criterion and optimizer\n",
    "import fc_model\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "images, labels = next(iter(trainloader))\n",
    "features_size = images.view(images.shape[0],-1).shape[1]\n",
    "print(f\"features_size = {features_size}\")\n",
    "\n",
    "network_model = fc_model.Network(features_size, 2, [512, 256, 128])\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(network_model.parameters(), lr=0.001)\n",
    "\n",
    "fc_model.train(\n",
    "    network_model,\n",
    "    trainloader,\n",
    "    testloader,\n",
    "    criterion,\n",
    "    optimizer=optimizer,\n",
    "    features_dim=features_size,\n",
    "    epochs=2)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
